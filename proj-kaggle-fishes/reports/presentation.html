<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Cristian, Mikael">
  <meta name="dcterms.date" content="2017-05-23">
  <title>Partage de connaissance</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
  <h1 class="title">Partage de connaissance</h1>
  <p class="subtitle">Compétition kaggle 🐟</p>
  <p class="author">Cristian, Mikael</p>
  <p class="date">2017-05-23</p>
</section>

<section><section id="the-competition-and-the-data" class="titleslide slide level1"><h1>1. The competition and the data</h1></section><section id="starting-point" class="slide level2" data-background="https://media.giphy.com/media/WfNIOjdnCh212/giphy.gif?response_id=591d519f2c191af3c3ffbedd">
<h2><font color="white">Starting point </font></h2>
<p><font color="white">In the Western and Central Pacific, 60% of the world’s tuna is caught illegally, a threat to marine ecosystem.</font></p>
</section><section id="goal-of-the-competition" class="slide level2">
<h2>Goal of the competition</h2>
<p>Automate fish detection on pictures from fishing boats. (with machine learning)</p>
<figure>
<embed src="https://kaggle2.blob.core.windows.net/competitions/kaggle/5568/media/TNC.mp4" title="opt title" class="video data-autoplay" style="width:80.0%" height="400" />
</figure>
</section><section id="images-classes" class="slide level2">
<h2>Images classes</h2>
<figure>
<img src="images/fish_classes.jpeg" class="plain" />
</figure>
</section><section id="data" class="slide level2">
<h2>Data</h2>
<table>
<thead>
<tr class="header">
<th>Name</th>
<th>Number of photos</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Train</td>
<td>3777</td>
</tr>
<tr class="even">
<td>Test stage 1</td>
<td>1000</td>
</tr>
<tr class="odd">
<td>Test stage 2</td>
<td>12000</td>
</tr>
</tbody>
</table>
</section><section id="class-distribution" class="slide level2">
<h2>Class distribution</h2>
<figure>
<img src="images/Distribution.png" class="plain" style="width:50.0%" />
</figure>
</section><section id="image-sizes" class="slide level2">
<h2>Image sizes</h2>
<figure>
<img src="images/Images_sizes.png" class="plain" style="width:50.0%" />
</figure>
</section><section id="preliminary-observations" class="slide level2">
<h2>Preliminary observations</h2>
<ul>
<li>Pictures from video sequences</li>
<li>Limited number of boats in training set</li>
<li>Day/night pictures</li>
<li>Multiple fishes per picture</li>
<li>Train set labelling errors</li>
</ul>
</section><section id="important-dates" class="slide level2">
<h2>Important dates</h2>
<table>
<thead>
<tr class="header">
<th>Stage</th>
<th>Date</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Competition start</td>
<td>14 Nov 2016</td>
</tr>
<tr class="even">
<td>We start 🎉</td>
<td>13 Jan 2017</td>
</tr>
<tr class="odd">
<td>End stage 1</td>
<td>6 April 2017</td>
</tr>
<tr class="even">
<td>End stage 2</td>
<td>13 April 2017</td>
</tr>
</tbody>
</table>
</section></section>
<section><section id="computer-vision-based-approach" class="titleslide slide level1"><h1>2. Computer vision based approach</h1></section><section id="extract-features" class="slide level2">
<h2>Extract features</h2>
<figure>
<img src="images/bag_of_features_1.png" class="plain" />
</figure>
</section><section id="combine-features-and-train-a-model" class="slide level2">
<h2>Combine features and train a model</h2>
<figure>
<img src="images/bag_of_features_2.png" class="plain" />
</figure>
</section><section id="analyze-the-results" class="slide level2">
<h2>Analyze the results</h2>
<figure>
<img src="images/results_I_bag_of_features.png" class="plain" />
</figure>
</section><section id="remove-background-information" class="slide level2">
<h2>Remove background information</h2>
<figure>
<img src="images/interest_point_detection.png" class="plain" style="width:70.0%" />
</figure>
</section><section id="scoring" class="slide level2">
<h2>Scoring</h2>
<figure>
<img src="images/scores_xgboost.svg" class="plain" style="width:70.0%" />
</figure>
</section></section>
<section><section id="a-methodological-break" class="titleslide slide level1"><h1>3. A methodological break</h1></section><section id="cookiecutter" class="slide level2">
<h2>Cookiecutter</h2>
<pre class="shell"><code>├── LICENSE
├── Makefile           &lt;- Makefile with commands like &#39;make data&#39; or &#39;make train&#39;
├── README.md          &lt;- The top-level README for developers using this project.
├── data
│   ├── external       &lt;- Data from third party sources.
│   ├── interim        &lt;- Intermediate data that has been transformed.
│   ├── processed      &lt;- The final, canonical data sets for modeling.
│   └── raw            &lt;- The original, immutable data dump.
│
├── docs               &lt;- A default Sphinx project; see sphinx-doc.org for details
│
├── models             &lt;- Trained and serialized models, model predictions, or model summaries
│
├── notebooks          &lt;- Jupyter notebooks. Naming convention is a number (for ordering),
│                         the creator s initials, and a short &#39;-&#39; delimited description, e.g.
│                         &#39;1.0-jqp-initial-data-exploration&#39;.
│
├── references         &lt;- Data dictionaries, manuals, and all other explanatory materials.
│
├── reports            &lt;- Generated analysis as HTML, PDF, LaTeX, etc.
│   └── figures        &lt;- Generated graphics and figures to be used in reporting
│
├── requirements.txt   &lt;- The requirements file for reproducing the analysis environment, e.g.
│                         generated with &#39;pip freeze &gt; requirements.txt&#39;
│
├── src                &lt;- Source code for use in this project.
│   ├── __init__.py    &lt;- Makes src a Python module
│   │
│   ├── data           &lt;- Scripts to download or generate data
│   │   └── make_dataset.py
│   │
│   ├── features       &lt;- Scripts to turn raw data into features for modeling
│   │   └── build_features.py
│   │
│   ├── models         &lt;- Scripts to train models and then use trained models to make
│   │   │                 predictions
│   │   ├── predict_model.py
│   │   └── train_model.py
│   │
│   └── visualization  &lt;- Scripts to create exploratory and results oriented visualizations
│       └── visualize.py
│
└── tox.ini            &lt;- tox file with settings for running tox; see tox.testrun.org
</code></pre>
</section><section id="data-abstraction-layer" class="slide level2">
<h2>Data abstraction layer</h2>
<p>Every picture was</p>
</section></section>
<section><section id="deep-learning-approach" class="titleslide slide level1"><h1>4. Deep learning approach</h1></section><section id="bounding-box-regression" class="slide level2">
<h2>Bounding box regression</h2>
<ul>
<li>Fishes Bounding box coordinates shared on <a href="https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/discussion/25902">kaggle forum</a>.</li>
<li>Done using <a href="https://github.com/cvhciKIT/sloth">Sloth</a>.</li>
<li>Coordinates of the bounding box referenced as (<code>x</code>, <code>y</code>, <code>width</code> and <code>height</code>).</li>
</ul>
</section><section id="multiple-fish-per-picture" class="slide level2">
<h2>Multiple fish per picture</h2>
<ul>
<li>Only one bounding box per picture,</li>
<li>Data augmentation by selecting multiple fishes per pictures</li>
<li>No Fish : empty coordinates.</li>
</ul>
</section><section id="image-preprocessing-with-keras" class="slide level2">
<h2>Image preprocessing with keras</h2>
<ul>
<li>Preprocessing with Keras (<a href="https://keras.io/preprocessing/image/">ImageDataGenerator</a>)
<ul>
<li>Rescale, rotation, shift, shear, flip, whitening, etc.</li>
</ul></li>
<li>Preprocessing generator <code>flow_from_directory</code>:
<ul>
<li>Read images form a directory.</li>
<li>Assign class for each subdirectory</li>
<li>Generates batches of augmented/normalized data</li>
<li>Yields batches indefinitely, in an infinite loop.</li>
</ul></li>
</ul>
</section><section id="training" class="slide level2">
<h2>Training</h2>
<ul>
<li>Keras also provides a method to train images by batches (<code>fit_generator</code>)
<ul>
<li>reduce memory utilization.</li>
<li>image preprocessing to be done in parallel of training process</li>
</ul></li>
<li>Requirement: bounding box coordinates and the Fish/NoFish label must be transformed as an iterator.</li>
</ul>
</section><section class="slide level2">

<h3 id="train-the-model-by-batch">Train the model by batch</h3>
<ul>
<li>The generator that feed the training fonction by batch contains:
<ul>
<li>The image generator</li>
<li>The bounding box coordinates generator</li>
<li>The Fish/NoFish label</li>
</ul></li>
<li>itertools: <a href="https://docs.python.org/2/library/itertools.html#itertools.cycle">cylce</a>, <a href="https://docs.python.org/2/library/itertools.html#itertools.izip">izip</a></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> itertools.cycle(<span class="st">&#39;ABCD&#39;</span>)
A B C D A B C D ...`
<span class="op">&gt;&gt;&gt;</span> itertools.izip(<span class="st">&#39;ABCD&#39;</span>, <span class="st">&#39;xy&#39;</span>)
Ax By`</code></pre></div>
</section><section id="pretrained-model" class="slide level2">
<h2>Pretrained model</h2>
<ul>
<li><p>Pretrained network determine universal features (curves and edges in its early layers).</p></li>
<li>Pretrained models
<ul>
<li>Complex architecture with huge amount of parametres</li>
<li>Trained on large datasets like the ImageNet, with 1.2M labelled images.</li>
</ul></li>
</ul>
</section><section id="fine-tuning" class="slide level2">
<h2>Fine tuning</h2>
<ul>
<li>Replace last layer it with a new softmax layer with the number of class</li>
</ul>
<figure>
<img src="images/transfer_learning.png" title="opt title" />
</figure>
</section><section id="image-augmentation" class="slide level2">
<h2>Image augmentation</h2>
<ul>
<li>Rescale <code>[0:255] -&gt; [0.:1.]</code>
<ul>
<li>InceptionV3 graph operates on floating point values</li>
</ul></li>
<li>shear</li>
</ul>
<p><img src="images/shear-before.jpeg" alt="shear" /> <img src="images/shear-after.jpeg" alt="shear after" /></p>
<ul>
<li>rotation, shift, shear, flip, whitening, etc.</li>
</ul>
</section></section>
<section><section id="elements-of-conclusion" class="titleslide slide level1"><h1>5. Elements of conclusion</h1></section><section class="slide level2">

<ul>
<li>Image preprocessing can significantly increase the performance of a classification algorithm.</li>
<li>A feature descriptor represents a simplified version of an image by extracting useful information and throwing away extraneous information.</li>
<li>Using feature description increases training speed compared with raw images.</li>
</ul>
</section></section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
              { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
