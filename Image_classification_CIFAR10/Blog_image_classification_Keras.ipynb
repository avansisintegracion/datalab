{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A toy convolutional neural network for image classification with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/dataset.png\">\n",
    "\n",
    "## Context - aim\n",
    "\n",
    "In our previous article - Image classification with a pre-trained deep neural network -, we introduced a quick guide on how to build an image classifier, using a pre-trained neural network to perform feature extraction and plugging it into a custom classifier that is specifically trained to perform image recognition on the dataset of interest.\n",
    "\n",
    "The present article is meant to unveil the details that are hidden inside the \"black box\" represented by a neural network built for image classification. We propose to build a basic convolutional neural network so as to grab the key concepts behind it, and at the same time become familiar with the Python [Keras](https://keras.io/) library for neural networks.\n",
    "\n",
    "In our previous article, the feature extraction was performed with the deep [Inception-V3](https://arxiv.org/abs/1512.00567) trained on the [ImageNet dataset](http://www.image-net.org/) which contains millions of images, yielding a classification accuracy of 95.4% on the ([Product Image Categorization Dataset](https://www.microsoft.com/en-us/research/people/xingx/)). Unfortunately, this dataset is no longer public, that's why here we propose to use the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) which is composed of 60000 32x32 RGB images, distributed among 10 classes, with 6000 images per class. Three years ago, a [Kaggle competition](https://www.kaggle.com/c/cifar-10) was held, aiming to yield the best classification accuracy on this dataset. The competition winner, [Ben Graham](http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/graham/), reached an accuracy of 95.5% by using a deep convolutional network the architecture of which is described [here](http://blog.kaggle.com/2015/01/02/cifar-10-competition-winners-interviews-with-dr-ben-graham-phil-culliton-zygmunt-zajac/).\n",
    "\n",
    "The architecture of the model introduced in this tutorial will be kept very basic on purpose, to allow the training and parameter tuning to be carried out on a regular computer using CPU. For this reasons, obviously our toy model won't allow us to get such an impressive performance, but let's see what accuracy we can reach!\n",
    "\n",
    "\n",
    "## Image recognition and neural networks\n",
    "\n",
    "Over the past few years, neural nets have proven to be very efficient as regards image classification. As an example, \"basic\" multilayer perceptrons can yield very good results in terms of classification errors on the well-known handwritten digits [MNIST dataset](http://yann.lecun.com/exdb/mnist/) (ref : [D. C. Ciresan et al. (2010)](https://arxiv.org/pdf/1003.0358.pdf))\n",
    "\n",
    "*To get familiar with neural networks (with a nice tutorial on the handwritten digits classification problem) : * http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "\n",
    "Object recognition using \"real life\" images can however prove to be tricky, and this for many reasons. A few are listed below :\n",
    "- The separation between the object and its background is not necessarily obvious\n",
    "- Several pictures of a same object can actually look quite different the one from the other. For example, the object's location in the image or the illumination can vary, which means the classification model needs to be invariant under certain transformations (translational symmetry for example)\n",
    "- Efficient computer vision requires models that are able to exploit the 2D topology of pixels, as well as locality.\n",
    "\n",
    "\n",
    "Because of their particular properties, convolutional neural networks (CNNs) allow to address the issues listed above.\n",
    "\n",
    "\n",
    "### Convolutional neural networks\n",
    "\n",
    "By construction, CNNs are well suited for image classification :\n",
    "- from one convolutional layer (CL) to the next one, only a few units are connected together, which allows local treatment of subsets of pixels\n",
    "- parameter sharing in one given CL contributes to translational invariance of the model\n",
    "- In practice, the two constraints listed above drastically reduce the number of model parameters to be computed, and then allow to train quite complex models in reasonable time.\n",
    "\n",
    "*Some useful references to gain knowledge of CNNs : * \n",
    "http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "\n",
    "A basic CNN consists in successions of convolutional layers (CL) and pooling layers (PL). Convolutional layers allow to extract several feature maps from the input images, while pooling layers perform some subsampling (i.e. dimensionality reduction) on the feature maps. Those successions of CLs and PLs correspond to a step of feature extraction. For image classification, the output layer is a fully connected NN layer with a number of units equal to the number of classes. The output layer activation is a softmax, so that the i$^{th}$ output unit activation is consistent with the probability that the image belongs to class i.\n",
    "\n",
    "\n",
    "It's also common to see in a CNN, the CLs and PLs being combined with some rectification (non-linearities) and normalization layers that can drastically improve the classification accuracy ([Jarrett et al. (2009)](http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf))\n",
    "\n",
    "The following scheme, taken from [M. Peemen et al. (2011)](http://parse.ele.tue.nl/system/attachments/11/original/paperspeedsigncnn.pdf?1305713044), represents a basic architecture with two convolution+subsampling steps plugged into a classifier :\n",
    "\n",
    "<img src=\"files/CNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building CNNs with Keras\n",
    "\n",
    "Below are loaded some useful libraries for building, training and evaluating neural nets.\n",
    "- [Keras](https://keras.io/) is a python library running either on [Tensorflow](https://www.tensorflow.org/) or [Theano](http://deeplearning.net/software/theano/). The following pieces of codes are valid for a Tensorflow implementation. [Here are some instructions to install Tensorflow](https://github.com/tensorflow/tensorflow#download-and-setup). As training neural nets can be quite computationally costly, it is recommended to install the gpu version of tensorflow (obviously, it's possible only if you have a dedicated GPU!).\n",
    "\n",
    "- In what's next we'll use some methods that are implemented in the well-known machine learning library [scikit-learn](http://scikit-learn.org/stable/). In particular, the methods [cross_val_score()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) and [RandomizedSearchCV()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) will be used, respectively to apply some unit tests to the models and to perform hyperparameter tuning. For those functions to be called on Keras models, those latter will be wrapped into classes that are \"compatible\" with scikit-learn. [Here is some useful tutorial to build scitkit-learn wrappers for estimators](http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Convolution2D, AveragePooling2D, MaxPooling2D, Flatten, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset\n",
    "The piece of code below allows to load the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) directly from the Keras library. To do so, the cifar10 module has to be imported from keras.datasets.\n",
    "\n",
    "In the following, the model architecture selection and hyperparameter tuning will be performed by 3-fold cross-validation on the train set (X_train, y_train). The test set is staged for performance assessment of the model chosen in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size :  50000\n",
      "Test set size :  10000\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('Train set size : ', X_train.shape[0])\n",
    "print('Test set size : ', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping input arrays\n",
    "Below the X_train and X_test arrays are reshaped to match 2D matrices. This is because some of the scikit-learn methods that will be used later (for example RandomizedSearchCV) assume this particular shape for the input features. The X_train and X_test 2D matrices will however be reshaped to take their original format (n_examples, height, width, n_channels) before being fed into the convolutional network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000 32 32 3\n",
      "(50000, 3072) (50000,)\n",
      "(10000, 3072) (10000,)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "n_labels = len(np.unique(y_train))\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "height= X_train.shape[1]\n",
    "width = X_train.shape[2]\n",
    "n_channels = X_train.shape[3]\n",
    "print(n_train, n_test, height, width, n_channels)\n",
    "\n",
    "X_train = X_train.reshape((n_train,height*width*n_channels))\n",
    "X_test = X_test.reshape((n_test,height*width*n_channels))\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "The pipeline to perform model selection is inspired from the reference article [Jarrett et al. (2009)](http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf).\n",
    "\n",
    "### Model architecture\n",
    "\n",
    "#### For feature extraction\n",
    "\n",
    "In this paper, the impact of the following model attributes on object classification accuracy is investigated :\n",
    "- number of convolutional layers (CL) needed to perform feature extraction\n",
    "- type of pooling (PL) used. There are typically two ways of reducing the feature dimensions : average pooling consists in keeping the mean value in a subset of the feature maps coming from the convolutional layers, whereas max pooling retains the maximum value instead.\n",
    "- role of rectification layers (RL), i.e. applying some non-linearities and at the same time rectifying the values out of the convolutions so that they keep being consistent with pixels (positive values).\n",
    "\n",
    "\n",
    "[Jarrett et al. (2009)](http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf) state that in general, the best suited architecture consists in using at least two successive convolution+pooling+rectification steps for feature extraction. In general, max pooling provides slightly better results than average pooling. As regards the rectification layers, many functions can be used but in what follows we'll stick to rectified linear units (ReLU), as those are the ones that are often the best suited for image classification.\n",
    "\n",
    "Still, we encourage you to play by yourself with those different parameters to test if the best model architecture that you find is consistent with what [Jarrett et al. (2009)](http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf) claims !\n",
    "\n",
    "#### For classification\n",
    "\n",
    "We'll first use the most basic classifier which consists in a single fully connected output layer with 10 units (because we have 10 classes) and a softmax activation function. This corresponds to a linear classifier. Then we'll add an hidden layer between the feature extractor and the output layer, leading to more complex decision boundaries.\n",
    "\n",
    "### Hyperparameter tuning\n",
    "\n",
    "Some hyperparameters tuning will be performed by using [randomized search](http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html). The concerned hyperparameters are :\n",
    "- number of feature maps in CLs\n",
    "- dimensions of feature maps in CLs\n",
    "- dimensions of pooling matrices in PLs\n",
    "- number of hidden units in layer between the feature extractor and the output layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a unit_test function to extract cross-validated score for model architecture selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unit_test(classifier, nb_iter=3):\n",
    "    test_size = 0.2\n",
    "    random_state = 15\n",
    "    cv = StratifiedShuffleSplit(y_train, nb_iter,test_size=test_size,random_state=random_state)\n",
    "    clf = classifier()\n",
    "    scores = cross_val_score(clf, X=X_train, y=y_train, scoring='accuracy', cv=cv)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a hyperparameter_optim function to perform randomized search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_optim(classifier, params, nb_iter=10, cv=3):\n",
    "\n",
    "    clf = RandomizedSearchCV(estimator=classifier(), param_distributions=params, n_iter=nb_iter, cv=cv, scoring='accuracy')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found:\")\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores:\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                % (mean, std * 2, params))\n",
    "    print()\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Basic\" model with only one convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building corresponding classifier inheriting from sklearn.BaseEstimator\n",
    "##### Default hyperparameters\n",
    "- nb_filters = 32, filter_size = (3,3) in CL\n",
    "- pool_size = (2,2) in PL\n",
    "- nb_epochs = 20\n",
    "\n",
    "##### Early stopping\n",
    "- An early stopping condition based on the monitoring of the validation set accuracy is used so as to avoid overfitting and improve a bit the training time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(BaseEstimator):  \n",
    "\n",
    "    def __init__(self, nb_filters=32, filter_size=3, pool_size=2):\n",
    "        self.nb_filters = nb_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.pool_size = pool_size\n",
    "        \n",
    "    def preprocess(self, X):\n",
    "        X = X.reshape((X.shape[0],height,width,n_channels))\n",
    "        X = (X / 255.)\n",
    "        X = X.astype(np.float32)\n",
    "        return X\n",
    "    \n",
    "    def preprocess_y(self, y):\n",
    "        return np_utils.to_categorical(y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = self.preprocess(X)\n",
    "        y = self.preprocess_y(y)\n",
    "        \n",
    "        hyper_parameters = dict(\n",
    "        nb_filters = self.nb_filters,\n",
    "        filter_size = self.filter_size,\n",
    "        pool_size = self.pool_size \n",
    "        )\n",
    "        \n",
    "        print(\"FIT PARAMS : \")\n",
    "        print(hyper_parameters)\n",
    "        \n",
    "        self.model = build_model(hyper_parameters)\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=1, verbose=1, mode='auto')\n",
    "        self.model.fit(X, y, nb_epoch=20, verbose=1, callbacks=[earlyStopping], validation_split=0.2, \n",
    "                       validation_data=None, shuffle=True)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict_classes(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        print(self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None))\n",
    "        return self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT PARAMS : \n",
      "{'filter_size': 3, 'nb_filters': 32, 'pool_size': 2}\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/20\n",
      "32000/32000 [==============================] - 17s - loss: 1.7516 - acc: 0.3930 - val_loss: 1.5888 - val_acc: 0.4462\n",
      "Epoch 2/20\n",
      "32000/32000 [==============================] - 18s - loss: 1.5220 - acc: 0.4780 - val_loss: 1.4962 - val_acc: 0.4794\n",
      "Epoch 3/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.4355 - acc: 0.5101 - val_loss: 1.4381 - val_acc: 0.5032\n",
      "Epoch 4/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.3750 - acc: 0.5319 - val_loss: 1.4049 - val_acc: 0.5084\n",
      "Epoch 5/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.3308 - acc: 0.5454 - val_loss: 1.3772 - val_acc: 0.5218\n",
      "Epoch 6/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.2904 - acc: 0.5600 - val_loss: 1.3328 - val_acc: 0.5353\n",
      "Epoch 7/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.2567 - acc: 0.5744 - val_loss: 1.3121 - val_acc: 0.5451\n",
      "Epoch 8/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.2274 - acc: 0.5844 - val_loss: 1.3096 - val_acc: 0.5447\n",
      "Epoch 9/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.2007 - acc: 0.5938 - val_loss: 1.3014 - val_acc: 0.5451\n",
      "Epoch 10/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.1724 - acc: 0.6037 - val_loss: 1.3070 - val_acc: 0.5434\n",
      "Epoch 11/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.1494 - acc: 0.6116 - val_loss: 1.2575 - val_acc: 0.5644\n",
      "Epoch 12/20\n",
      "32000/32000 [==============================] - 18s - loss: 1.1271 - acc: 0.6188 - val_loss: 1.2597 - val_acc: 0.5604\n",
      "Epoch 13/20\n",
      "31968/32000 [============================>.] - ETA: 0s - loss: 1.1066 - acc: 0.6267Epoch 00012: early stopping\n",
      "32000/32000 [==============================] - 18s - loss: 1.1065 - acc: 0.6267 - val_loss: 1.2619 - val_acc: 0.5666\n",
      "10000/10000 [==============================] - 2s     \n",
      "FIT PARAMS : \n",
      "{'filter_size': 3, 'nb_filters': 32, 'pool_size': 2}\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/20\n",
      "32000/32000 [==============================] - 18s - loss: 1.7577 - acc: 0.3850 - val_loss: 1.5987 - val_acc: 0.4490\n",
      "Epoch 2/20\n",
      "32000/32000 [==============================] - 18s - loss: 1.5278 - acc: 0.4743 - val_loss: 1.5181 - val_acc: 0.4765\n",
      "Epoch 3/20\n",
      "32000/32000 [==============================] - 18s - loss: 1.4436 - acc: 0.5034 - val_loss: 1.4481 - val_acc: 0.4980\n",
      "Epoch 4/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.3803 - acc: 0.5285 - val_loss: 1.4193 - val_acc: 0.5074\n",
      "Epoch 5/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.3340 - acc: 0.5452 - val_loss: 1.4052 - val_acc: 0.5194\n",
      "Epoch 6/20\n",
      "32000/32000 [==============================] - 18s - loss: 1.2944 - acc: 0.5579 - val_loss: 1.3550 - val_acc: 0.5298\n",
      "Epoch 7/20\n",
      "32000/32000 [==============================] - 18s - loss: 1.2596 - acc: 0.5728 - val_loss: 1.3691 - val_acc: 0.5244\n",
      "Epoch 8/20\n",
      "32000/32000 [==============================] - 20s - loss: 1.2284 - acc: 0.5826 - val_loss: 1.3033 - val_acc: 0.5536\n",
      "Epoch 9/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.2021 - acc: 0.5919 - val_loss: 1.2859 - val_acc: 0.5607\n",
      "Epoch 10/20\n",
      "32000/32000 [==============================] - 20s - loss: 1.1748 - acc: 0.6013 - val_loss: 1.2893 - val_acc: 0.5621\n",
      "Epoch 11/20\n",
      "32000/32000 [==============================] - 20s - loss: 1.1529 - acc: 0.6090 - val_loss: 1.2692 - val_acc: 0.5667\n",
      "Epoch 12/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.1313 - acc: 0.6167 - val_loss: 1.2381 - val_acc: 0.5733\n",
      "Epoch 13/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.1095 - acc: 0.6278 - val_loss: 1.2558 - val_acc: 0.5650\n",
      "Epoch 14/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.0897 - acc: 0.6338 - val_loss: 1.2297 - val_acc: 0.5764\n",
      "Epoch 15/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.0729 - acc: 0.6378 - val_loss: 1.2203 - val_acc: 0.5811\n",
      "Epoch 16/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.0562 - acc: 0.6428 - val_loss: 1.2240 - val_acc: 0.5783\n",
      "Epoch 17/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.0440 - acc: 0.6491 - val_loss: 1.2123 - val_acc: 0.5881\n",
      "Epoch 18/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.0267 - acc: 0.6535 - val_loss: 1.2075 - val_acc: 0.5880\n",
      "Epoch 19/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.0120 - acc: 0.6616 - val_loss: 1.2301 - val_acc: 0.5780\n",
      "Epoch 20/20\n",
      "32000/32000 [==============================] - 20s - loss: 0.9999 - acc: 0.6614 - val_loss: 1.1978 - val_acc: 0.5909\n",
      " 9984/10000 [============================>.] - ETA: 0sFIT PARAMS : \n",
      "{'filter_size': 3, 'nb_filters': 32, 'pool_size': 2}\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.7669 - acc: 0.3806 - val_loss: 1.6133 - val_acc: 0.4400\n",
      "Epoch 2/20\n",
      "32000/32000 [==============================] - 20s - loss: 1.5348 - acc: 0.4733 - val_loss: 1.5107 - val_acc: 0.4781\n",
      "Epoch 3/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.4445 - acc: 0.5052 - val_loss: 1.4498 - val_acc: 0.5005\n",
      "Epoch 4/20\n",
      "32000/32000 [==============================] - 18s - loss: 1.3838 - acc: 0.5265 - val_loss: 1.4367 - val_acc: 0.4984\n",
      "Epoch 5/20\n",
      "32000/32000 [==============================] - 18s - loss: 1.3352 - acc: 0.5435 - val_loss: 1.3975 - val_acc: 0.5130\n",
      "Epoch 6/20\n",
      "32000/32000 [==============================] - 19s - loss: 1.2933 - acc: 0.5608 - val_loss: 1.3551 - val_acc: 0.5324\n",
      "Epoch 7/20\n",
      "32000/32000 [==============================] - 18s - loss: 1.2585 - acc: 0.5755 - val_loss: 1.3343 - val_acc: 0.5366\n",
      "Epoch 8/20\n",
      "32000/32000 [==============================] - 18s - loss: 1.2272 - acc: 0.5807 - val_loss: 1.3123 - val_acc: 0.5507\n",
      "Epoch 9/20\n",
      "32000/32000 [==============================] - 18s - loss: 1.1975 - acc: 0.5955 - val_loss: 1.3146 - val_acc: 0.5450\n",
      "Epoch 10/20\n",
      "31968/32000 [============================>.] - ETA: 0s - loss: 1.1718 - acc: 0.6017Epoch 00009: early stopping\n",
      "32000/32000 [==============================] - 18s - loss: 1.1718 - acc: 0.6018 - val_loss: 1.3202 - val_acc: 0.5460\n",
      " 9984/10000 [============================>.] - ETA: 0s[ 0.5693  0.6001  0.5364]\n"
     ]
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    net = Sequential()\n",
    "    net.add(Convolution2D(hp['nb_filters'], hp['filter_size'], hp['filter_size'], border_mode='same', \n",
    "                          input_shape=(height,width,n_channels)))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size'],hp['pool_size'])))\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(output_dim=n_labels))\n",
    "    net.add(Activation(\"softmax\"))\n",
    "    \n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    "\n",
    "print(unit_test(Classifier,nb_iter=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "This first very basic model yields a classification accuracy of 55-60%, while using max pooling. You can try to replace max by average pooling. You'll notice that the latter yields poorer results (accuracy about 45-50%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with two convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building corresponding classifier inheriting from sklearn.BaseEstimator\n",
    "##### Default hyperparameters\n",
    "- nb_filters_1 = 32, filter_size_1 = (3,3) in 1st CL\n",
    "- pool_size_1 = (2,2) in 1st PL\n",
    "- nb_filters_2 = 32, filter_size_2 = (3,3) in 2nd CL\n",
    "- pool_size_2 = (2,2) in 2nd PL\n",
    "- nb_epochs = 10\n",
    "\n",
    "##### Early stopping\n",
    "- An early stopping condition based on the monitoring of the validation set accuracy is used so as to avoid overfitting and improve a bit the training time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(BaseEstimator):  \n",
    "\n",
    "    def __init__(self, nb_filters_1=32, filter_size_1=3, pool_size_1=2,\n",
    "                 nb_filters_2=32, filter_size_2=3, pool_size_2=2):\n",
    "        self.nb_filters_1 = nb_filters_1\n",
    "        self.filter_size_1 = filter_size_1\n",
    "        self.pool_size_1 = pool_size_1\n",
    "        self.nb_filters_2 = nb_filters_2\n",
    "        self.filter_size_2 = filter_size_2\n",
    "        self.pool_size_2 = pool_size_2\n",
    "        \n",
    "    def preprocess(self, X):\n",
    "        X = X.reshape((X.shape[0],height,width,n_channels))\n",
    "        X = (X / 255.)\n",
    "        X = X.astype(np.float32)\n",
    "        return X\n",
    "    \n",
    "    def preprocess_y(self, y):\n",
    "        return np_utils.to_categorical(y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = self.preprocess(X)\n",
    "        y = self.preprocess_y(y)\n",
    "        \n",
    "        hyper_parameters = dict(\n",
    "        nb_filters_1 = self.nb_filters_1,\n",
    "        filter_size_1 = self.filter_size_1,\n",
    "        pool_size_1 = self.pool_size_1,\n",
    "        nb_filters_2 = self.nb_filters_2,\n",
    "        filter_size_2 = self.filter_size_2,\n",
    "        pool_size_2 = self.pool_size_2\n",
    "        )\n",
    "        \n",
    "        print(\"FIT PARAMS : \")\n",
    "        print(hyper_parameters)\n",
    "        \n",
    "        self.model = build_model(hyper_parameters)\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=1, verbose=1, mode='auto')\n",
    "        self.model.fit(X, y, nb_epoch=20, verbose=1, callbacks=[earlyStopping], validation_split=0.2, \n",
    "                       validation_data=None, shuffle=True)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"PREDICT\")\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict_classes(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        print(\"SCORE\")\n",
    "        print(self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None))\n",
    "        return self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT PARAMS : \n",
      "{'nb_filters_1': 32, 'nb_filters_2': 32, 'filter_size_2': 3, 'pool_size_1': 2, 'pool_size_2': 2, 'filter_size_1': 3}\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/20\n",
      "32000/32000 [==============================] - 31s - loss: 1.7576 - acc: 0.3715 - val_loss: 1.5059 - val_acc: 0.4630\n",
      "Epoch 2/20\n",
      "32000/32000 [==============================] - 34s - loss: 1.4076 - acc: 0.5016 - val_loss: 1.3599 - val_acc: 0.5192\n",
      "Epoch 3/20\n",
      "32000/32000 [==============================] - 34s - loss: 1.3000 - acc: 0.5455 - val_loss: 1.2798 - val_acc: 0.5515\n",
      "Epoch 4/20\n",
      "32000/32000 [==============================] - 33s - loss: 1.2352 - acc: 0.5673 - val_loss: 1.2303 - val_acc: 0.5656\n",
      "Epoch 5/20\n",
      "32000/32000 [==============================] - 34s - loss: 1.1805 - acc: 0.5882 - val_loss: 1.2055 - val_acc: 0.5796\n",
      "Epoch 6/20\n",
      "32000/32000 [==============================] - 33s - loss: 1.1315 - acc: 0.6050 - val_loss: 1.1782 - val_acc: 0.5869\n",
      "Epoch 7/20\n",
      "32000/32000 [==============================] - 33s - loss: 1.0867 - acc: 0.6222 - val_loss: 1.2378 - val_acc: 0.5641\n",
      "Epoch 8/20\n",
      "32000/32000 [==============================] - 33s - loss: 1.0483 - acc: 0.6351 - val_loss: 1.1084 - val_acc: 0.6195\n",
      "Epoch 9/20\n",
      "32000/32000 [==============================] - 32s - loss: 1.0127 - acc: 0.6489 - val_loss: 1.0528 - val_acc: 0.6386\n",
      "Epoch 10/20\n",
      "32000/32000 [==============================] - 32s - loss: 0.9812 - acc: 0.6623 - val_loss: 1.0667 - val_acc: 0.6336\n",
      "Epoch 11/20\n",
      "32000/32000 [==============================] - 32s - loss: 0.9555 - acc: 0.6716 - val_loss: 1.0524 - val_acc: 0.6380\n",
      "Epoch 12/20\n",
      "32000/32000 [==============================] - 34s - loss: 0.9313 - acc: 0.6809 - val_loss: 1.0174 - val_acc: 0.6489\n",
      "Epoch 13/20\n",
      "32000/32000 [==============================] - 32s - loss: 0.9122 - acc: 0.6871 - val_loss: 1.0066 - val_acc: 0.6501\n",
      "Epoch 14/20\n",
      "32000/32000 [==============================] - 33s - loss: 0.8917 - acc: 0.6940 - val_loss: 1.0008 - val_acc: 0.6542\n",
      "Epoch 15/20\n",
      "32000/32000 [==============================] - 32s - loss: 0.8764 - acc: 0.6982 - val_loss: 0.9740 - val_acc: 0.6674\n",
      "Epoch 16/20\n",
      "32000/32000 [==============================] - 32s - loss: 0.8615 - acc: 0.7040 - val_loss: 0.9913 - val_acc: 0.6614\n",
      "Epoch 17/20\n",
      "31968/32000 [============================>.] - ETA: 0s - loss: 0.8463 - acc: 0.7093Epoch 00016: early stopping\n",
      "32000/32000 [==============================] - 32s - loss: 0.8463 - acc: 0.7093 - val_loss: 0.9857 - val_acc: 0.6617\n",
      "PREDICT\n",
      " 9984/10000 [============================>.] - ETA: 0sFIT PARAMS : \n",
      "{'nb_filters_1': 32, 'nb_filters_2': 32, 'filter_size_2': 3, 'pool_size_1': 2, 'pool_size_2': 2, 'filter_size_1': 3}\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/20\n",
      "32000/32000 [==============================] - 32s - loss: 1.7536 - acc: 0.3698 - val_loss: 1.5114 - val_acc: 0.4666\n",
      "Epoch 2/20\n",
      "32000/32000 [==============================] - 32s - loss: 1.4284 - acc: 0.4974 - val_loss: 1.3638 - val_acc: 0.5169\n",
      "Epoch 3/20\n",
      "32000/32000 [==============================] - 33s - loss: 1.3071 - acc: 0.5388 - val_loss: 1.3368 - val_acc: 0.5320\n",
      "Epoch 4/20\n",
      "32000/32000 [==============================] - 33s - loss: 1.2330 - acc: 0.5669 - val_loss: 1.2179 - val_acc: 0.5720\n",
      "Epoch 5/20\n",
      "32000/32000 [==============================] - 33s - loss: 1.1791 - acc: 0.5880 - val_loss: 1.1824 - val_acc: 0.5864\n",
      "Epoch 6/20\n",
      "32000/32000 [==============================] - 33s - loss: 1.1320 - acc: 0.6054 - val_loss: 1.1389 - val_acc: 0.6030\n",
      "Epoch 7/20\n",
      "32000/32000 [==============================] - 33s - loss: 1.0916 - acc: 0.6199 - val_loss: 1.1162 - val_acc: 0.6115\n",
      "Epoch 8/20\n",
      "32000/32000 [==============================] - 33s - loss: 1.0542 - acc: 0.6348 - val_loss: 1.0958 - val_acc: 0.6182\n",
      "Epoch 9/20\n",
      "32000/32000 [==============================] - 33s - loss: 1.0210 - acc: 0.6494 - val_loss: 1.0811 - val_acc: 0.6222\n",
      "Epoch 10/20\n",
      "32000/32000 [==============================] - 33s - loss: 0.9926 - acc: 0.6586 - val_loss: 1.0475 - val_acc: 0.6381\n",
      "Epoch 11/20\n",
      "32000/32000 [==============================] - 33s - loss: 0.9696 - acc: 0.6677 - val_loss: 1.0190 - val_acc: 0.6455\n",
      "Epoch 12/20\n",
      "32000/32000 [==============================] - 33s - loss: 0.9483 - acc: 0.6764 - val_loss: 1.0182 - val_acc: 0.6498\n",
      "Epoch 13/20\n",
      "32000/32000 [==============================] - 33s - loss: 0.9283 - acc: 0.6817 - val_loss: 1.0060 - val_acc: 0.6515\n",
      "Epoch 14/20\n",
      "32000/32000 [==============================] - 33s - loss: 0.9108 - acc: 0.6875 - val_loss: 1.0013 - val_acc: 0.6539\n",
      "Epoch 15/20\n",
      "32000/32000 [==============================] - 33s - loss: 0.8948 - acc: 0.6943 - val_loss: 0.9938 - val_acc: 0.6580\n",
      "Epoch 16/20\n",
      "32000/32000 [==============================] - 34s - loss: 0.8800 - acc: 0.6978 - val_loss: 0.9883 - val_acc: 0.6600\n",
      "Epoch 17/20\n",
      "32000/32000 [==============================] - 33s - loss: 0.8674 - acc: 0.7039 - val_loss: 0.9798 - val_acc: 0.6606\n",
      "Epoch 18/20\n",
      "32000/32000 [==============================] - 33s - loss: 0.8521 - acc: 0.7106 - val_loss: 0.9822 - val_acc: 0.6636\n",
      "Epoch 19/20\n",
      "31968/32000 [============================>.] - ETA: 0s - loss: 0.8428 - acc: 0.7131Epoch 00018: early stopping\n",
      "32000/32000 [==============================] - 33s - loss: 0.8430 - acc: 0.7130 - val_loss: 0.9969 - val_acc: 0.6591\n",
      "PREDICT\n",
      " 9984/10000 [============================>.] - ETA: 0sFIT PARAMS : \n",
      "{'nb_filters_1': 32, 'nb_filters_2': 32, 'filter_size_2': 3, 'pool_size_1': 2, 'pool_size_2': 2, 'filter_size_1': 3}\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/20\n",
      "32000/32000 [==============================] - 34s - loss: 1.7391 - acc: 0.3784 - val_loss: 1.4957 - val_acc: 0.4746\n",
      "Epoch 2/20\n",
      "32000/32000 [==============================] - 34s - loss: 1.4143 - acc: 0.4995 - val_loss: 1.3805 - val_acc: 0.5106\n",
      "Epoch 3/20\n",
      "32000/32000 [==============================] - 34s - loss: 1.3089 - acc: 0.5402 - val_loss: 1.3321 - val_acc: 0.5459\n",
      "Epoch 4/20\n",
      "32000/32000 [==============================] - 34s - loss: 1.2424 - acc: 0.5636 - val_loss: 1.2504 - val_acc: 0.5664\n",
      "Epoch 5/20\n",
      "32000/32000 [==============================] - 35s - loss: 1.1885 - acc: 0.5843 - val_loss: 1.2087 - val_acc: 0.5767\n",
      "Epoch 6/20\n",
      "32000/32000 [==============================] - 35s - loss: 1.1427 - acc: 0.6008 - val_loss: 1.1668 - val_acc: 0.5929\n",
      "Epoch 7/20\n",
      "32000/32000 [==============================] - 36s - loss: 1.1018 - acc: 0.6153 - val_loss: 1.1992 - val_acc: 0.5846\n",
      "Epoch 8/20\n",
      "32000/32000 [==============================] - 36s - loss: 1.0660 - acc: 0.6281 - val_loss: 1.1367 - val_acc: 0.6034\n",
      "Epoch 9/20\n",
      "32000/32000 [==============================] - 35s - loss: 1.0361 - acc: 0.6394 - val_loss: 1.1129 - val_acc: 0.6180\n",
      "Epoch 10/20\n",
      "32000/32000 [==============================] - 34s - loss: 1.0074 - acc: 0.6514 - val_loss: 1.1010 - val_acc: 0.6189\n",
      "Epoch 11/20\n",
      "32000/32000 [==============================] - 34s - loss: 0.9821 - acc: 0.6602 - val_loss: 1.0964 - val_acc: 0.6208\n",
      "Epoch 12/20\n",
      "32000/32000 [==============================] - 35s - loss: 0.9591 - acc: 0.6679 - val_loss: 1.0595 - val_acc: 0.6355\n",
      "Epoch 13/20\n",
      "32000/32000 [==============================] - 35s - loss: 0.9377 - acc: 0.6756 - val_loss: 1.0501 - val_acc: 0.6328\n",
      "Epoch 14/20\n",
      "32000/32000 [==============================] - 34s - loss: 0.9179 - acc: 0.6842 - val_loss: 1.0442 - val_acc: 0.6442\n",
      "Epoch 15/20\n",
      "32000/32000 [==============================] - 33s - loss: 0.8996 - acc: 0.6891 - val_loss: 1.0372 - val_acc: 0.6399\n",
      "Epoch 16/20\n",
      "32000/32000 [==============================] - 34s - loss: 0.8849 - acc: 0.6935 - val_loss: 1.0153 - val_acc: 0.6542\n",
      "Epoch 17/20\n",
      "32000/32000 [==============================] - 34s - loss: 0.8686 - acc: 0.7004 - val_loss: 1.0586 - val_acc: 0.6356\n",
      "Epoch 18/20\n",
      "31968/32000 [============================>.] - ETA: 0s - loss: 0.8562 - acc: 0.7069Epoch 00017: early stopping\n",
      "32000/32000 [==============================] - 35s - loss: 0.8561 - acc: 0.7069 - val_loss: 1.0160 - val_acc: 0.6577\n",
      "PREDICT\n",
      " 9984/10000 [============================>.] - ETA: 0s[ 0.6667  0.6661  0.6623]\n"
     ]
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    net = Sequential()\n",
    "    net.add(Convolution2D(hp['nb_filters_1'], hp['filter_size_1'], hp['filter_size_1'], border_mode='same', \n",
    "                          input_shape=(height,width,n_channels)))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_1'],hp['pool_size_1'])))\n",
    "    net.add(Convolution2D(hp['nb_filters_2'], hp['filter_size_2'], hp['filter_size_2'], border_mode='same'))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_2'],hp['pool_size_2'])))\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(output_dim=n_labels))\n",
    "    net.add(Activation(\"softmax\"))\n",
    "    \n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    "\n",
    "print(unit_test(Classifier,nb_iter=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "With two-steps convolutions, we go up to 66% for classification accuracy. Here the last step consists in applying a linear classifier to the features maps. Let's try to complexify the decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a hidden layer between CLs and output : allowing non-linear separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(BaseEstimator):  \n",
    "\n",
    "    def __init__(self, nb_filters_1=32, filter_size_1=3, pool_size_1=2,\n",
    "                 nb_filters_2=32, filter_size_2=3, pool_size_2=2, nb_hunits=400):\n",
    "        self.nb_filters_1 = nb_filters_1\n",
    "        self.filter_size_1 = filter_size_1\n",
    "        self.pool_size_1 = pool_size_1\n",
    "        self.nb_filters_2 = nb_filters_2\n",
    "        self.filter_size_2 = filter_size_2\n",
    "        self.pool_size_2 = pool_size_2\n",
    "        self.nb_hunits = nb_hunits\n",
    "        \n",
    "    def preprocess(self, X):\n",
    "        X = X.reshape((X.shape[0],height,width,n_channels))\n",
    "        X = (X / 255.)\n",
    "        X = X.astype(np.float32)\n",
    "        return X\n",
    "    \n",
    "    def preprocess_y(self, y):\n",
    "        return np_utils.to_categorical(y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = self.preprocess(X)\n",
    "        y = self.preprocess_y(y)\n",
    "        \n",
    "        hyper_parameters = dict(\n",
    "        nb_filters_1 = self.nb_filters_1,\n",
    "        filter_size_1 = self.filter_size_1,\n",
    "        pool_size_1 = self.pool_size_1,\n",
    "        nb_filters_2 = self.nb_filters_2,\n",
    "        filter_size_2 = self.filter_size_2,\n",
    "        pool_size_2 = self.pool_size_2,\n",
    "        nb_hunits = self.nb_hunits\n",
    "        )\n",
    "        \n",
    "        print(\"FIT PARAMS : \")\n",
    "        print(hyper_parameters)\n",
    "        \n",
    "        self.model = build_model(hyper_parameters)\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=1, verbose=1, mode='auto')\n",
    "        self.model.fit(X, y, nb_epoch=20, verbose=1, callbacks=[earlyStopping], validation_split=0.2, \n",
    "                       validation_data=None, shuffle=True)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"PREDICT\")\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict_classes(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        print(\"SCORE\")\n",
    "        print(self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None))\n",
    "        return self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT PARAMS : \n",
      "{'nb_filters_1': 32, 'nb_filters_2': 32, 'nb_hunits': 400, 'filter_size_2': 3, 'pool_size_1': 2, 'pool_size_2': 2, 'filter_size_1': 3}\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/20\n",
      "32000/32000 [==============================] - 42s - loss: 1.6479 - acc: 0.4016 - val_loss: 1.4339 - val_acc: 0.4794\n",
      "Epoch 2/20\n",
      "32000/32000 [==============================] - 44s - loss: 1.2421 - acc: 0.5578 - val_loss: 1.2168 - val_acc: 0.5701\n",
      "Epoch 3/20\n",
      "32000/32000 [==============================] - 44s - loss: 1.0869 - acc: 0.6187 - val_loss: 1.0674 - val_acc: 0.6270\n",
      "Epoch 4/20\n",
      "32000/32000 [==============================] - 43s - loss: 0.9757 - acc: 0.6600 - val_loss: 1.0353 - val_acc: 0.6358\n",
      "Epoch 5/20\n",
      "32000/32000 [==============================] - 43s - loss: 0.8879 - acc: 0.6921 - val_loss: 0.9591 - val_acc: 0.6683\n",
      "Epoch 6/20\n",
      "32000/32000 [==============================] - 44s - loss: 0.8054 - acc: 0.7210 - val_loss: 0.9536 - val_acc: 0.6684\n",
      "Epoch 7/20\n",
      "32000/32000 [==============================] - 43s - loss: 0.7335 - acc: 0.7477 - val_loss: 0.9340 - val_acc: 0.6764\n",
      "Epoch 8/20\n",
      "32000/32000 [==============================] - 43s - loss: 0.6657 - acc: 0.7720 - val_loss: 0.9672 - val_acc: 0.6745\n",
      "Epoch 9/20\n",
      "31968/32000 [============================>.] - ETA: 0s - loss: 0.5940 - acc: 0.7995Epoch 00008: early stopping\n",
      "32000/32000 [==============================] - 43s - loss: 0.5940 - acc: 0.7995 - val_loss: 0.9679 - val_acc: 0.6750\n",
      "PREDICT\n",
      "10000/10000 [==============================] - 3s     \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 32, 'nb_filters_2': 32, 'nb_hunits': 400, 'filter_size_2': 3, 'pool_size_1': 2, 'pool_size_2': 2, 'filter_size_1': 3}\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/20\n",
      "32000/32000 [==============================] - 43s - loss: 1.6675 - acc: 0.3960 - val_loss: 1.3937 - val_acc: 0.5005\n",
      "Epoch 2/20\n",
      "32000/32000 [==============================] - 44s - loss: 1.2814 - acc: 0.5435 - val_loss: 1.1986 - val_acc: 0.5690\n",
      "Epoch 3/20\n",
      "32000/32000 [==============================] - 44s - loss: 1.1220 - acc: 0.6040 - val_loss: 1.1487 - val_acc: 0.5930\n",
      "Epoch 4/20\n",
      "32000/32000 [==============================] - 44s - loss: 1.0181 - acc: 0.6435 - val_loss: 1.0671 - val_acc: 0.6166\n",
      "Epoch 5/20\n",
      "32000/32000 [==============================] - 44s - loss: 0.9285 - acc: 0.6745 - val_loss: 1.0190 - val_acc: 0.6406\n",
      "Epoch 6/20\n",
      "32000/32000 [==============================] - 44s - loss: 0.8496 - acc: 0.7041 - val_loss: 0.9743 - val_acc: 0.6584\n",
      "Epoch 7/20\n",
      "32000/32000 [==============================] - 44s - loss: 0.7721 - acc: 0.7318 - val_loss: 0.9295 - val_acc: 0.6753\n",
      "Epoch 8/20\n",
      "32000/32000 [==============================] - 45s - loss: 0.7044 - acc: 0.7564 - val_loss: 0.9612 - val_acc: 0.6747\n",
      "Epoch 9/20\n",
      "31968/32000 [============================>.] - ETA: 0s - loss: 0.6333 - acc: 0.7829Epoch 00008: early stopping\n",
      "32000/32000 [==============================] - 44s - loss: 0.6333 - acc: 0.7829 - val_loss: 0.9415 - val_acc: 0.6864\n",
      "PREDICT\n",
      " 9984/10000 [============================>.] - ETA: 0sFIT PARAMS : \n",
      "{'nb_filters_1': 32, 'nb_filters_2': 32, 'nb_hunits': 400, 'filter_size_2': 3, 'pool_size_1': 2, 'pool_size_2': 2, 'filter_size_1': 3}\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/20\n",
      "32000/32000 [==============================] - 45s - loss: 1.6588 - acc: 0.4004 - val_loss: 1.3857 - val_acc: 0.5101\n",
      "Epoch 2/20\n",
      "32000/32000 [==============================] - 45s - loss: 1.2773 - acc: 0.5467 - val_loss: 1.2761 - val_acc: 0.5511\n",
      "Epoch 3/20\n",
      "32000/32000 [==============================] - 45s - loss: 1.1293 - acc: 0.5994 - val_loss: 1.1988 - val_acc: 0.5765\n",
      "Epoch 4/20\n",
      "32000/32000 [==============================] - 45s - loss: 1.0165 - acc: 0.6418 - val_loss: 1.0530 - val_acc: 0.6338\n",
      "Epoch 5/20\n",
      "32000/32000 [==============================] - 45s - loss: 0.9237 - acc: 0.6755 - val_loss: 1.0526 - val_acc: 0.6338\n",
      "Epoch 6/20\n",
      "32000/32000 [==============================] - 45s - loss: 0.8453 - acc: 0.7045 - val_loss: 1.0142 - val_acc: 0.6524\n",
      "Epoch 7/20\n",
      "32000/32000 [==============================] - 45s - loss: 0.7717 - acc: 0.7321 - val_loss: 0.9965 - val_acc: 0.6589\n",
      "Epoch 8/20\n",
      "32000/32000 [==============================] - 45s - loss: 0.6989 - acc: 0.7550 - val_loss: 0.9720 - val_acc: 0.6676\n",
      "Epoch 9/20\n",
      "32000/32000 [==============================] - 44s - loss: 0.6295 - acc: 0.7839 - val_loss: 1.0012 - val_acc: 0.6714\n",
      "Epoch 10/20\n",
      "31968/32000 [============================>.] - ETA: 0s - loss: 0.5632 - acc: 0.8058Epoch 00009: early stopping\n",
      "32000/32000 [==============================] - 45s - loss: 0.5631 - acc: 0.8058 - val_loss: 1.0321 - val_acc: 0.6687\n",
      "PREDICT\n",
      " 9984/10000 [============================>.] - ETA: 0s[ 0.6783  0.6753  0.6699]\n"
     ]
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    net = Sequential()\n",
    "    net.add(Convolution2D(hp['nb_filters_1'], hp['filter_size_1'], hp['filter_size_1'], border_mode='same', \n",
    "                          input_shape=(height,width,n_channels)))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_1'],hp['pool_size_1'])))\n",
    "    net.add(Convolution2D(hp['nb_filters_2'], hp['filter_size_2'], hp['filter_size_2'], border_mode='same'))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_2'],hp['pool_size_2'])))\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(output_dim=hp['nb_hunits']))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(Dense(output_dim=n_labels))\n",
    "    net.add(Activation(\"softmax\"))\n",
    "    \n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    "\n",
    "print(unit_test(Classifier,nb_iter=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "The accuracy is a bit improved (from 66% to 67%) by adding the fully connected hidden layer. Here we have arbitrarily fixed the number of hidden units to 400. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization\n",
    "In the following cell, we'll use randomized search to tune some hyperparameters :\n",
    "- number of hidden units in the classifier\n",
    "- number and size of the convolution filters\n",
    "- size of pooling matrices\n",
    "\n",
    "This is quite time consuming (about 10 hours on a laptop with CPU), that's why we advise you not to run directly this cell, but rather copy its content into a .py script that will be launch independantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT PARAMS : \n",
      "{'nb_filters_1': 64, 'nb_filters_2': 64, 'nb_hunits': 54, 'filter_size_2': 3, 'pool_size_1': 2, 'pool_size_2': 2, 'filter_size_1': 3}\n",
      "Train on 26666 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26666/26666 [==============================] - 69s - loss: 1.7976 - acc: 0.3452 - val_loss: 1.6459 - val_acc: 0.4120\n",
      "Epoch 2/20\n",
      "26666/26666 [==============================] - 71s - loss: 1.3846 - acc: 0.5056 - val_loss: 1.4104 - val_acc: 0.4939\n",
      "Epoch 3/20\n",
      "26666/26666 [==============================] - 71s - loss: 1.2308 - acc: 0.5638 - val_loss: 1.2499 - val_acc: 0.5596\n",
      "Epoch 4/20\n",
      "26666/26666 [==============================] - 70s - loss: 1.1314 - acc: 0.5997 - val_loss: 1.3136 - val_acc: 0.5439\n",
      "Epoch 5/20\n",
      "26656/26666 [============================>.] - ETA: 0s - loss: 1.0490 - acc: 0.6299Epoch 00004: early stopping\n",
      "26666/26666 [==============================] - 74s - loss: 1.0491 - acc: 0.6299 - val_loss: 1.2617 - val_acc: 0.5626\n",
      "PREDICT\n",
      "16667/16667 [==============================] - 14s    \n",
      "PREDICT\n",
      "33333/33333 [==============================] - 28s    \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 64, 'nb_filters_2': 64, 'nb_hunits': 54, 'filter_size_2': 3, 'pool_size_1': 2, 'pool_size_2': 2, 'filter_size_1': 3}\n",
      "Train on 26666 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26666/26666 [==============================] - 71s - loss: 1.7333 - acc: 0.3745 - val_loss: 1.4469 - val_acc: 0.4818\n",
      "Epoch 2/20\n",
      "26666/26666 [==============================] - 73s - loss: 1.3263 - acc: 0.5308 - val_loss: 1.2988 - val_acc: 0.5400\n",
      "Epoch 3/20\n",
      "26666/26666 [==============================] - 73s - loss: 1.1922 - acc: 0.5804 - val_loss: 1.1722 - val_acc: 0.5889\n",
      "Epoch 4/20\n",
      "26666/26666 [==============================] - 75s - loss: 1.0951 - acc: 0.6164 - val_loss: 1.1558 - val_acc: 0.5926\n",
      "Epoch 5/20\n",
      "26666/26666 [==============================] - 75s - loss: 1.0177 - acc: 0.6434 - val_loss: 1.0672 - val_acc: 0.6262\n",
      "Epoch 6/20\n",
      "26666/26666 [==============================] - 73s - loss: 0.9480 - acc: 0.6681 - val_loss: 1.0527 - val_acc: 0.6375\n",
      "Epoch 7/20\n",
      "26666/26666 [==============================] - 73s - loss: 0.8929 - acc: 0.6887 - val_loss: 1.0566 - val_acc: 0.6252\n",
      "Epoch 8/20\n",
      "26666/26666 [==============================] - 75s - loss: 0.8412 - acc: 0.7072 - val_loss: 0.9774 - val_acc: 0.6618\n",
      "Epoch 9/20\n",
      "26666/26666 [==============================] - 76s - loss: 0.7958 - acc: 0.7211 - val_loss: 1.0508 - val_acc: 0.6321\n",
      "Epoch 10/20\n",
      "26666/26666 [==============================] - 75s - loss: 0.7553 - acc: 0.7382 - val_loss: 0.9727 - val_acc: 0.6699\n",
      "Epoch 11/20\n",
      "26666/26666 [==============================] - 73s - loss: 0.7128 - acc: 0.7530 - val_loss: 0.9774 - val_acc: 0.6663\n",
      "Epoch 12/20\n",
      "26656/26666 [============================>.] - ETA: 0s - loss: 0.6770 - acc: 0.7641Epoch 00011: early stopping\n",
      "26666/26666 [==============================] - 72s - loss: 0.6769 - acc: 0.7642 - val_loss: 1.0716 - val_acc: 0.6492\n",
      "PREDICT\n",
      "16667/16667 [==============================] - 13s    \n",
      "PREDICT\n",
      "33333/33333 [==============================] - 27s    \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 64, 'nb_filters_2': 64, 'nb_hunits': 54, 'filter_size_2': 3, 'pool_size_1': 2, 'pool_size_2': 2, 'filter_size_1': 3}\n",
      "Train on 26667 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26667/26667 [==============================] - 84s - loss: 1.7496 - acc: 0.3667 - val_loss: 1.5359 - val_acc: 0.4654\n",
      "Epoch 2/20\n",
      "26667/26667 [==============================] - 90s - loss: 1.3325 - acc: 0.5248 - val_loss: 1.3682 - val_acc: 0.5062\n",
      "Epoch 3/20\n",
      "26667/26667 [==============================] - 78s - loss: 1.1956 - acc: 0.5802 - val_loss: 1.2770 - val_acc: 0.5421\n",
      "Epoch 4/20\n",
      "26667/26667 [==============================] - 77s - loss: 1.0970 - acc: 0.6131 - val_loss: 1.1671 - val_acc: 0.5968\n",
      "Epoch 5/20\n",
      "26667/26667 [==============================] - 74s - loss: 1.0217 - acc: 0.6430 - val_loss: 1.0777 - val_acc: 0.6213\n",
      "Epoch 6/20\n",
      "26667/26667 [==============================] - 72s - loss: 0.9520 - acc: 0.6657 - val_loss: 1.0986 - val_acc: 0.6162\n",
      "Epoch 7/20\n",
      "26667/26667 [==============================] - 74s - loss: 0.8911 - acc: 0.6898 - val_loss: 1.0645 - val_acc: 0.6304\n",
      "Epoch 8/20\n",
      "26667/26667 [==============================] - 76s - loss: 0.8387 - acc: 0.7065 - val_loss: 1.1173 - val_acc: 0.6174\n",
      "Epoch 9/20\n",
      "26656/26667 [============================>.] - ETA: 0s - loss: 0.7883 - acc: 0.7262Epoch 00008: early stopping\n",
      "26667/26667 [==============================] - 76s - loss: 0.7883 - acc: 0.7263 - val_loss: 1.1217 - val_acc: 0.6234\n",
      "PREDICT\n",
      "16666/16666 [==============================] - 14s    \n",
      "PREDICT\n",
      "33334/33334 [==============================] - 28s    \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 16, 'nb_filters_2': 32, 'nb_hunits': 251, 'filter_size_2': 6, 'pool_size_1': 2, 'pool_size_2': 4, 'filter_size_1': 3}\n",
      "Train on 26666 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26666/26666 [==============================] - 37s - loss: 1.7606 - acc: 0.3635 - val_loss: 1.4945 - val_acc: 0.4573\n",
      "Epoch 2/20\n",
      "26666/26666 [==============================] - 35s - loss: 1.3545 - acc: 0.5177 - val_loss: 1.4604 - val_acc: 0.4825\n",
      "Epoch 3/20\n",
      "26666/26666 [==============================] - 34s - loss: 1.2032 - acc: 0.5768 - val_loss: 1.1938 - val_acc: 0.5802\n",
      "Epoch 4/20\n",
      "26666/26666 [==============================] - 35s - loss: 1.1069 - acc: 0.6100 - val_loss: 1.1500 - val_acc: 0.5934\n",
      "Epoch 5/20\n",
      "26666/26666 [==============================] - 38s - loss: 1.0376 - acc: 0.6405 - val_loss: 1.0697 - val_acc: 0.6240\n",
      "Epoch 6/20\n",
      "26666/26666 [==============================] - 35s - loss: 0.9769 - acc: 0.6618 - val_loss: 1.1100 - val_acc: 0.6160\n",
      "Epoch 7/20\n",
      "26656/26666 [============================>.] - ETA: 0s - loss: 0.9277 - acc: 0.6803Epoch 00006: early stopping\n",
      "26666/26666 [==============================] - 35s - loss: 0.9277 - acc: 0.6803 - val_loss: 1.0841 - val_acc: 0.6217\n",
      "PREDICT\n",
      "16667/16667 [==============================] - 7s     \n",
      "PREDICT\n",
      "33333/33333 [==============================] - 15s    \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 16, 'nb_filters_2': 32, 'nb_hunits': 251, 'filter_size_2': 6, 'pool_size_1': 2, 'pool_size_2': 4, 'filter_size_1': 3}\n",
      "Train on 26666 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26666/26666 [==============================] - 34s - loss: 1.7261 - acc: 0.3750 - val_loss: 1.4803 - val_acc: 0.4702\n",
      "Epoch 2/20\n",
      "26666/26666 [==============================] - 34s - loss: 1.3186 - acc: 0.5288 - val_loss: 1.3785 - val_acc: 0.4959\n",
      "Epoch 3/20\n",
      "26666/26666 [==============================] - 34s - loss: 1.1778 - acc: 0.5860 - val_loss: 1.1713 - val_acc: 0.5845\n",
      "Epoch 4/20\n",
      "26666/26666 [==============================] - 36s - loss: 1.0851 - acc: 0.6195 - val_loss: 1.1042 - val_acc: 0.6145\n",
      "Epoch 5/20\n",
      "26666/26666 [==============================] - 37s - loss: 1.0157 - acc: 0.6445 - val_loss: 1.0634 - val_acc: 0.6291\n",
      "Epoch 6/20\n",
      "26666/26666 [==============================] - 36s - loss: 0.9595 - acc: 0.6634 - val_loss: 1.0268 - val_acc: 0.6474\n",
      "Epoch 7/20\n",
      "26666/26666 [==============================] - 36s - loss: 0.9132 - acc: 0.6792 - val_loss: 1.0591 - val_acc: 0.6387\n",
      "Epoch 8/20\n",
      "26666/26666 [==============================] - 36s - loss: 0.8727 - acc: 0.6955 - val_loss: 0.9824 - val_acc: 0.6585\n",
      "Epoch 9/20\n",
      "26666/26666 [==============================] - 37s - loss: 0.8324 - acc: 0.7096 - val_loss: 0.9623 - val_acc: 0.6720\n",
      "Epoch 10/20\n",
      "26666/26666 [==============================] - 37s - loss: 0.7970 - acc: 0.7213 - val_loss: 1.0199 - val_acc: 0.6430\n",
      "Epoch 11/20\n",
      "26666/26666 [==============================] - 37s - loss: 0.7621 - acc: 0.7334 - val_loss: 0.9382 - val_acc: 0.6783\n",
      "Epoch 12/20\n",
      "26666/26666 [==============================] - 36s - loss: 0.7298 - acc: 0.7465 - val_loss: 0.9444 - val_acc: 0.6802\n",
      "Epoch 13/20\n",
      "26656/26666 [============================>.] - ETA: 0s - loss: 0.7007 - acc: 0.7574Epoch 00012: early stopping\n",
      "26666/26666 [==============================] - 36s - loss: 0.7007 - acc: 0.7574 - val_loss: 1.0443 - val_acc: 0.6448\n",
      "PREDICT\n",
      "16667/16667 [==============================] - 7s     \n",
      "PREDICT\n",
      "33333/33333 [==============================] - 15s    \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 16, 'nb_filters_2': 32, 'nb_hunits': 251, 'filter_size_2': 6, 'pool_size_1': 2, 'pool_size_2': 4, 'filter_size_1': 3}\n",
      "Train on 26667 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26667/26667 [==============================] - 36s - loss: 1.7405 - acc: 0.3692 - val_loss: 1.6645 - val_acc: 0.4063\n",
      "Epoch 2/20\n",
      "26667/26667 [==============================] - 36s - loss: 1.3362 - acc: 0.5223 - val_loss: 1.3432 - val_acc: 0.5185\n",
      "Epoch 3/20\n",
      "26667/26667 [==============================] - 36s - loss: 1.1956 - acc: 0.5766 - val_loss: 1.2369 - val_acc: 0.5578\n",
      "Epoch 4/20\n",
      "26667/26667 [==============================] - 36s - loss: 1.1032 - acc: 0.6100 - val_loss: 1.1902 - val_acc: 0.5827\n",
      "Epoch 5/20\n",
      "26667/26667 [==============================] - 36s - loss: 1.0323 - acc: 0.6381 - val_loss: 1.0837 - val_acc: 0.6225\n",
      "Epoch 6/20\n",
      "26667/26667 [==============================] - 36s - loss: 0.9751 - acc: 0.6575 - val_loss: 1.4113 - val_acc: 0.5211\n",
      "Epoch 7/20\n",
      "26667/26667 [==============================] - 34s - loss: 0.9299 - acc: 0.6762 - val_loss: 1.0752 - val_acc: 0.6261\n",
      "Epoch 8/20\n",
      "26667/26667 [==============================] - 38s - loss: 0.8868 - acc: 0.6882 - val_loss: 1.1039 - val_acc: 0.6093\n",
      "Epoch 9/20\n",
      "26667/26667 [==============================] - 36s - loss: 0.8489 - acc: 0.7052 - val_loss: 1.0531 - val_acc: 0.6357\n",
      "Epoch 10/20\n",
      "26667/26667 [==============================] - 36s - loss: 0.8171 - acc: 0.7120 - val_loss: 1.0328 - val_acc: 0.6525\n",
      "Epoch 11/20\n",
      "26667/26667 [==============================] - 35s - loss: 0.7825 - acc: 0.7277 - val_loss: 1.0014 - val_acc: 0.6592\n",
      "Epoch 12/20\n",
      "26667/26667 [==============================] - 36s - loss: 0.7536 - acc: 0.7386 - val_loss: 1.0250 - val_acc: 0.6553\n",
      "Epoch 13/20\n",
      "26656/26667 [============================>.] - ETA: 0s - loss: 0.7219 - acc: 0.7515Epoch 00012: early stopping\n",
      "26667/26667 [==============================] - 36s - loss: 0.7220 - acc: 0.7514 - val_loss: 1.1321 - val_acc: 0.6208\n",
      "PREDICT\n",
      "16666/16666 [==============================] - 8s     \n",
      "PREDICT\n",
      "33334/33334 [==============================] - 15s    \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 16, 'nb_filters_2': 16, 'nb_hunits': 881, 'filter_size_2': 3, 'pool_size_1': 2, 'pool_size_2': 2, 'filter_size_1': 3}\n",
      "Train on 26666 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26666/26666 [==============================] - 27s - loss: 1.6807 - acc: 0.3940 - val_loss: 1.4572 - val_acc: 0.4756\n",
      "Epoch 2/20\n",
      "26666/26666 [==============================] - 25s - loss: 1.3091 - acc: 0.5341 - val_loss: 1.3268 - val_acc: 0.5199\n",
      "Epoch 3/20\n",
      "26666/26666 [==============================] - 25s - loss: 1.1739 - acc: 0.5859 - val_loss: 1.2315 - val_acc: 0.5649\n",
      "Epoch 4/20\n",
      "26666/26666 [==============================] - 25s - loss: 1.0781 - acc: 0.6227 - val_loss: 1.1622 - val_acc: 0.5863\n",
      "Epoch 5/20\n",
      "26666/26666 [==============================] - 26s - loss: 0.9931 - acc: 0.6530 - val_loss: 1.1736 - val_acc: 0.5850\n",
      "Epoch 6/20\n",
      "26666/26666 [==============================] - 25s - loss: 0.9165 - acc: 0.6800 - val_loss: 1.1082 - val_acc: 0.6102\n",
      "Epoch 7/20\n",
      "26666/26666 [==============================] - 25s - loss: 0.8413 - acc: 0.7104 - val_loss: 1.1312 - val_acc: 0.6076\n",
      "Epoch 8/20\n",
      "26666/26666 [==============================] - 25s - loss: 0.7669 - acc: 0.7370 - val_loss: 1.0828 - val_acc: 0.6276\n",
      "Epoch 9/20\n",
      "26666/26666 [==============================] - 25s - loss: 0.6994 - acc: 0.7594 - val_loss: 1.0627 - val_acc: 0.6363\n",
      "Epoch 10/20\n",
      "26666/26666 [==============================] - 26s - loss: 0.6284 - acc: 0.7859 - val_loss: 1.1268 - val_acc: 0.6285\n",
      "Epoch 11/20\n",
      "26656/26666 [============================>.] - ETA: 0s - loss: 0.5578 - acc: 0.8081Epoch 00010: early stopping\n",
      "26666/26666 [==============================] - 25s - loss: 0.5579 - acc: 0.8081 - val_loss: 1.0687 - val_acc: 0.6561\n",
      "PREDICT\n",
      "16640/16667 [============================>.] - ETA: 0sPREDICT\n",
      "33333/33333 [==============================] - 7s     \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 16, 'nb_filters_2': 16, 'nb_hunits': 881, 'filter_size_2': 3, 'pool_size_1': 2, 'pool_size_2': 2, 'filter_size_1': 3}\n",
      "Train on 26666 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26666/26666 [==============================] - 25s - loss: 1.7063 - acc: 0.3832 - val_loss: 1.4658 - val_acc: 0.4822\n",
      "Epoch 2/20\n",
      "26666/26666 [==============================] - 25s - loss: 1.3312 - acc: 0.5274 - val_loss: 1.2681 - val_acc: 0.5521\n",
      "Epoch 3/20\n",
      "26666/26666 [==============================] - 25s - loss: 1.1897 - acc: 0.5773 - val_loss: 1.3990 - val_acc: 0.5116\n",
      "Epoch 4/20\n",
      "26666/26666 [==============================] - 25s - loss: 1.0928 - acc: 0.6160 - val_loss: 1.1463 - val_acc: 0.5950\n",
      "Epoch 5/20\n",
      "26666/26666 [==============================] - 26s - loss: 1.0058 - acc: 0.6454 - val_loss: 1.1288 - val_acc: 0.6021\n",
      "Epoch 6/20\n",
      "26666/26666 [==============================] - 25s - loss: 0.9300 - acc: 0.6742 - val_loss: 1.0623 - val_acc: 0.6286\n",
      "Epoch 7/20\n",
      "26666/26666 [==============================] - 25s - loss: 0.8582 - acc: 0.6994 - val_loss: 1.1187 - val_acc: 0.6142\n",
      "Epoch 8/20\n",
      "26656/26666 [============================>.] - ETA: 0s - loss: 0.7879 - acc: 0.7266Epoch 00007: early stopping\n",
      "26666/26666 [==============================] - 25s - loss: 0.7878 - acc: 0.7267 - val_loss: 1.0702 - val_acc: 0.6349\n",
      "PREDICT\n",
      "16667/16667 [==============================] - 4s     \n",
      "PREDICT\n",
      "33333/33333 [==============================] - 7s     \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 16, 'nb_filters_2': 16, 'nb_hunits': 881, 'filter_size_2': 3, 'pool_size_1': 2, 'pool_size_2': 2, 'filter_size_1': 3}\n",
      "Train on 26667 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26667/26667 [==============================] - 25s - loss: 1.7038 - acc: 0.3861 - val_loss: 1.4678 - val_acc: 0.4785\n",
      "Epoch 2/20\n",
      "26667/26667 [==============================] - 25s - loss: 1.3106 - acc: 0.5306 - val_loss: 1.3260 - val_acc: 0.5313\n",
      "Epoch 3/20\n",
      "26667/26667 [==============================] - 25s - loss: 1.1620 - acc: 0.5876 - val_loss: 1.2081 - val_acc: 0.5722\n",
      "Epoch 4/20\n",
      "26667/26667 [==============================] - 26s - loss: 1.0629 - acc: 0.6261 - val_loss: 1.1450 - val_acc: 0.5934\n",
      "Epoch 5/20\n",
      "26667/26667 [==============================] - 26s - loss: 0.9789 - acc: 0.6567 - val_loss: 1.2066 - val_acc: 0.5686\n",
      "Epoch 6/20\n",
      "26667/26667 [==============================] - 26s - loss: 0.9057 - acc: 0.6849 - val_loss: 1.1353 - val_acc: 0.6024\n",
      "Epoch 7/20\n",
      "26667/26667 [==============================] - 25s - loss: 0.8322 - acc: 0.7087 - val_loss: 1.0530 - val_acc: 0.6396\n",
      "Epoch 8/20\n",
      "26667/26667 [==============================] - 26s - loss: 0.7636 - acc: 0.7344 - val_loss: 1.0765 - val_acc: 0.6295\n",
      "Epoch 9/20\n",
      "26656/26667 [============================>.] - ETA: 0s - loss: 0.6967 - acc: 0.7596Epoch 00008: early stopping\n",
      "26667/26667 [==============================] - 26s - loss: 0.6967 - acc: 0.7596 - val_loss: 1.1111 - val_acc: 0.6217\n",
      "PREDICT\n",
      "16640/16666 [============================>.] - ETA: 0sPREDICT\n",
      "33334/33334 [==============================] - 7s     \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 32, 'nb_filters_2': 32, 'nb_hunits': 922, 'filter_size_2': 6, 'pool_size_1': 2, 'pool_size_2': 2, 'filter_size_1': 3}\n",
      "Train on 26666 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26666/26666 [==============================] - 83s - loss: 1.6797 - acc: 0.3910 - val_loss: 1.5166 - val_acc: 0.4468\n",
      "Epoch 2/20\n",
      "26666/26666 [==============================] - 81s - loss: 1.2307 - acc: 0.5645 - val_loss: 1.3506 - val_acc: 0.5236\n",
      "Epoch 3/20\n",
      "26666/26666 [==============================] - 80s - loss: 1.0318 - acc: 0.6358 - val_loss: 1.1849 - val_acc: 0.5868\n",
      "Epoch 4/20\n",
      "26666/26666 [==============================] - 81s - loss: 0.9075 - acc: 0.6825 - val_loss: 1.0856 - val_acc: 0.6262\n",
      "Epoch 5/20\n",
      "26666/26666 [==============================] - 81s - loss: 0.8039 - acc: 0.7192 - val_loss: 1.0851 - val_acc: 0.6175\n",
      "Epoch 6/20\n",
      "26666/26666 [==============================] - 82s - loss: 0.7083 - acc: 0.7560 - val_loss: 1.0010 - val_acc: 0.6600\n",
      "Epoch 7/20\n",
      "26666/26666 [==============================] - 81s - loss: 0.6145 - acc: 0.7909 - val_loss: 1.1154 - val_acc: 0.6381\n",
      "Epoch 8/20\n",
      "26666/26666 [==============================] - 83s - loss: 0.5234 - acc: 0.8235 - val_loss: 0.9165 - val_acc: 0.7062\n",
      "Epoch 9/20\n",
      "26666/26666 [==============================] - 80s - loss: 0.4330 - acc: 0.8562 - val_loss: 0.9823 - val_acc: 0.6855\n",
      "Epoch 10/20\n",
      "26656/26666 [============================>.] - ETA: 0s - loss: 0.3504 - acc: 0.8852Epoch 00009: early stopping\n",
      "26666/26666 [==============================] - 83s - loss: 0.3503 - acc: 0.8852 - val_loss: 0.9431 - val_acc: 0.7149\n",
      "PREDICT\n",
      "16667/16667 [==============================] - 13s    \n",
      "PREDICT\n",
      "33333/33333 [==============================] - 25s    \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 32, 'nb_filters_2': 32, 'nb_hunits': 922, 'filter_size_2': 6, 'pool_size_1': 2, 'pool_size_2': 2, 'filter_size_1': 3}\n",
      "Train on 26666 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26666/26666 [==============================] - 81s - loss: 1.6764 - acc: 0.3923 - val_loss: 1.4075 - val_acc: 0.4899\n",
      "Epoch 2/20\n",
      "26666/26666 [==============================] - 83s - loss: 1.2545 - acc: 0.5543 - val_loss: 1.2355 - val_acc: 0.5584\n",
      "Epoch 3/20\n",
      "26666/26666 [==============================] - 82s - loss: 1.0733 - acc: 0.6215 - val_loss: 1.1151 - val_acc: 0.6069\n",
      "Epoch 4/20\n",
      "26666/26666 [==============================] - 84s - loss: 0.9413 - acc: 0.6704 - val_loss: 1.0018 - val_acc: 0.6432\n",
      "Epoch 5/20\n",
      "26666/26666 [==============================] - 85s - loss: 0.8265 - acc: 0.7156 - val_loss: 0.9940 - val_acc: 0.6553\n",
      "Epoch 6/20\n",
      "26666/26666 [==============================] - 80s - loss: 0.7255 - acc: 0.7478 - val_loss: 1.0100 - val_acc: 0.6586\n",
      "Epoch 7/20\n",
      "26666/26666 [==============================] - 80s - loss: 0.6290 - acc: 0.7826 - val_loss: 0.9205 - val_acc: 0.6838\n",
      "Epoch 8/20\n",
      "26666/26666 [==============================] - 81s - loss: 0.5328 - acc: 0.8183 - val_loss: 0.9658 - val_acc: 0.6811\n",
      "Epoch 9/20\n",
      "26656/26666 [============================>.] - ETA: 0s - loss: 0.4405 - acc: 0.8517Epoch 00008: early stopping\n",
      "26666/26666 [==============================] - 80s - loss: 0.4407 - acc: 0.8516 - val_loss: 1.1535 - val_acc: 0.6525\n",
      "PREDICT\n",
      "16667/16667 [==============================] - 13s    \n",
      "PREDICT\n",
      "33333/33333 [==============================] - 26s    \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 32, 'nb_filters_2': 32, 'nb_hunits': 922, 'filter_size_2': 6, 'pool_size_1': 2, 'pool_size_2': 2, 'filter_size_1': 3}\n",
      "Train on 26667 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26667/26667 [==============================] - 82s - loss: 1.6580 - acc: 0.3965 - val_loss: 1.3999 - val_acc: 0.4894\n",
      "Epoch 2/20\n",
      "26667/26667 [==============================] - 83s - loss: 1.2294 - acc: 0.5606 - val_loss: 1.3065 - val_acc: 0.5349\n",
      "Epoch 3/20\n",
      "26667/26667 [==============================] - 82s - loss: 1.0484 - acc: 0.6282 - val_loss: 1.1459 - val_acc: 0.5835\n",
      "Epoch 4/20\n",
      "26667/26667 [==============================] - 81s - loss: 0.9187 - acc: 0.6768 - val_loss: 1.1374 - val_acc: 0.6147\n",
      "Epoch 5/20\n",
      "26667/26667 [==============================] - 80s - loss: 0.8145 - acc: 0.7169 - val_loss: 0.9922 - val_acc: 0.6567\n",
      "Epoch 6/20\n",
      "26667/26667 [==============================] - 80s - loss: 0.7186 - acc: 0.7482 - val_loss: 0.9930 - val_acc: 0.6615\n",
      "Epoch 7/20\n",
      "26656/26667 [============================>.] - ETA: 0s - loss: 0.6281 - acc: 0.7836Epoch 00006: early stopping\n",
      "26667/26667 [==============================] - 80s - loss: 0.6282 - acc: 0.7835 - val_loss: 1.1492 - val_acc: 0.6225\n",
      "PREDICT\n",
      "16666/16666 [==============================] - 13s    \n",
      "PREDICT\n",
      "33334/33334 [==============================] - 27s    \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 64, 'nb_filters_2': 16, 'nb_hunits': 209, 'filter_size_2': 9, 'pool_size_1': 2, 'pool_size_2': 4, 'filter_size_1': 3}\n",
      "Train on 26666 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26666/26666 [==============================] - 129s - loss: 1.7489 - acc: 0.3621 - val_loss: 1.5035 - val_acc: 0.4581\n",
      "Epoch 2/20\n",
      "26666/26666 [==============================] - 130s - loss: 1.3615 - acc: 0.5087 - val_loss: 1.3883 - val_acc: 0.4966\n",
      "Epoch 3/20\n",
      "26666/26666 [==============================] - 130s - loss: 1.2321 - acc: 0.5609 - val_loss: 1.2666 - val_acc: 0.5481\n",
      "Epoch 4/20\n",
      "26666/26666 [==============================] - 130s - loss: 1.1536 - acc: 0.5907 - val_loss: 1.1736 - val_acc: 0.5826\n",
      "Epoch 5/20\n",
      "26666/26666 [==============================] - 130s - loss: 1.0843 - acc: 0.6182 - val_loss: 1.2356 - val_acc: 0.5577\n",
      "Epoch 6/20\n",
      "26666/26666 [==============================] - 130s - loss: 1.0345 - acc: 0.6391 - val_loss: 1.0737 - val_acc: 0.6250\n",
      "Epoch 7/20\n",
      "26666/26666 [==============================] - 129s - loss: 0.9855 - acc: 0.6556 - val_loss: 1.1241 - val_acc: 0.6024\n",
      "Epoch 8/20\n",
      "26656/26666 [============================>.] - ETA: 0s - loss: 0.9458 - acc: 0.6677Epoch 00007: early stopping\n",
      "26666/26666 [==============================] - 130s - loss: 0.9457 - acc: 0.6677 - val_loss: 1.0801 - val_acc: 0.6217\n",
      "PREDICT\n",
      "16667/16667 [==============================] - 25s    \n",
      "PREDICT\n",
      "33333/33333 [==============================] - 52s    \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 64, 'nb_filters_2': 16, 'nb_hunits': 209, 'filter_size_2': 9, 'pool_size_1': 2, 'pool_size_2': 4, 'filter_size_1': 3}\n",
      "Train on 26666 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26666/26666 [==============================] - 134s - loss: 1.7786 - acc: 0.3510 - val_loss: 1.4548 - val_acc: 0.4783\n",
      "Epoch 2/20\n",
      "26666/26666 [==============================] - 137s - loss: 1.4009 - acc: 0.4995 - val_loss: 1.4986 - val_acc: 0.4780\n",
      "Epoch 3/20\n",
      "26666/26666 [==============================] - 133s - loss: 1.2716 - acc: 0.5456 - val_loss: 1.3271 - val_acc: 0.5188\n",
      "Epoch 4/20\n",
      "26666/26666 [==============================] - 135s - loss: 1.1842 - acc: 0.5796 - val_loss: 1.2519 - val_acc: 0.5595\n",
      "Epoch 5/20\n",
      "26666/26666 [==============================] - 134s - loss: 1.1120 - acc: 0.6078 - val_loss: 1.2556 - val_acc: 0.5520\n",
      "Epoch 6/20\n",
      "26666/26666 [==============================] - 134s - loss: 1.0537 - acc: 0.6288 - val_loss: 1.1344 - val_acc: 0.6040\n",
      "Epoch 7/20\n",
      "26666/26666 [==============================] - 133s - loss: 1.0022 - acc: 0.6496 - val_loss: 1.0558 - val_acc: 0.6304\n",
      "Epoch 8/20\n",
      "26666/26666 [==============================] - 132s - loss: 0.9588 - acc: 0.6640 - val_loss: 1.1352 - val_acc: 0.6066\n",
      "Epoch 9/20\n",
      "26666/26666 [==============================] - 132s - loss: 0.9216 - acc: 0.6788 - val_loss: 1.0278 - val_acc: 0.6367\n",
      "Epoch 10/20\n",
      "26666/26666 [==============================] - 132s - loss: 0.8854 - acc: 0.6894 - val_loss: 1.0408 - val_acc: 0.6405\n",
      "Epoch 11/20\n",
      "26656/26666 [============================>.] - ETA: 0s - loss: 0.8507 - acc: 0.7046Epoch 00010: early stopping\n",
      "26666/26666 [==============================] - 134s - loss: 0.8506 - acc: 0.7046 - val_loss: 1.0497 - val_acc: 0.6379\n",
      "PREDICT\n",
      "16667/16667 [==============================] - 27s    \n",
      "PREDICT\n",
      "33333/33333 [==============================] - 52s    \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 64, 'nb_filters_2': 16, 'nb_hunits': 209, 'filter_size_2': 9, 'pool_size_1': 2, 'pool_size_2': 4, 'filter_size_1': 3}\n",
      "Train on 26667 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26667/26667 [==============================] - 132s - loss: 1.7534 - acc: 0.3583 - val_loss: 1.5416 - val_acc: 0.4341\n",
      "Epoch 2/20\n",
      "26667/26667 [==============================] - 133s - loss: 1.3455 - acc: 0.5155 - val_loss: 1.3765 - val_acc: 0.5005\n",
      "Epoch 3/20\n",
      "26667/26667 [==============================] - 134s - loss: 1.2114 - acc: 0.5662 - val_loss: 1.4175 - val_acc: 0.5182\n",
      "Epoch 4/20\n",
      "26667/26667 [==============================] - 133s - loss: 1.1209 - acc: 0.6028 - val_loss: 1.1823 - val_acc: 0.5779\n",
      "Epoch 5/20\n",
      "26667/26667 [==============================] - 140s - loss: 1.0509 - acc: 0.6274 - val_loss: 1.1581 - val_acc: 0.5910\n",
      "Epoch 6/20\n",
      "26667/26667 [==============================] - 141s - loss: 0.9965 - acc: 0.6501 - val_loss: 1.1447 - val_acc: 0.6061\n",
      "Epoch 7/20\n",
      "26667/26667 [==============================] - 133s - loss: 0.9465 - acc: 0.6667 - val_loss: 1.0635 - val_acc: 0.6253\n",
      "Epoch 8/20\n",
      "26667/26667 [==============================] - 133s - loss: 0.9023 - acc: 0.6826 - val_loss: 1.1056 - val_acc: 0.6124\n",
      "Epoch 9/20\n",
      "26656/26667 [============================>.] - ETA: 0s - loss: 0.8650 - acc: 0.6935Epoch 00008: early stopping\n",
      "26667/26667 [==============================] - 134s - loss: 0.8651 - acc: 0.6935 - val_loss: 1.0994 - val_acc: 0.6199\n",
      "PREDICT\n",
      "16666/16666 [==============================] - 26s    \n",
      "PREDICT\n",
      "33334/33334 [==============================] - 53s    \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 32, 'nb_filters_2': 32, 'nb_hunits': 750, 'filter_size_2': 9, 'pool_size_1': 2, 'pool_size_2': 4, 'filter_size_1': 3}\n",
      "Train on 26666 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26666/26666 [==============================] - 103s - loss: 1.6930 - acc: 0.3864 - val_loss: 1.3620 - val_acc: 0.5076\n",
      "Epoch 2/20\n",
      "26666/26666 [==============================] - 103s - loss: 1.2572 - acc: 0.5499 - val_loss: 1.3007 - val_acc: 0.5358\n",
      "Epoch 3/20\n",
      "26666/26666 [==============================] - 104s - loss: 1.0986 - acc: 0.6094 - val_loss: 1.1107 - val_acc: 0.6123\n",
      "Epoch 4/20\n",
      "26666/26666 [==============================] - 105s - loss: 0.9957 - acc: 0.6500 - val_loss: 1.1890 - val_acc: 0.5934\n",
      "Epoch 5/20\n",
      "26666/26666 [==============================] - 105s - loss: 0.9107 - acc: 0.6824 - val_loss: 1.0323 - val_acc: 0.6448\n",
      "Epoch 6/20\n",
      "26666/26666 [==============================] - 103s - loss: 0.8394 - acc: 0.7074 - val_loss: 1.0711 - val_acc: 0.6303\n",
      "Epoch 7/20\n",
      "26656/26666 [============================>.] - ETA: 0s - loss: 0.7786 - acc: 0.7285Epoch 00006: early stopping\n",
      "26666/26666 [==============================] - 103s - loss: 0.7785 - acc: 0.7285 - val_loss: 1.0701 - val_acc: 0.6388\n",
      "PREDICT\n",
      "16667/16667 [==============================] - 20s    \n",
      "PREDICT\n",
      "33333/33333 [==============================] - 40s    \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 32, 'nb_filters_2': 32, 'nb_hunits': 750, 'filter_size_2': 9, 'pool_size_1': 2, 'pool_size_2': 4, 'filter_size_1': 3}\n",
      "Train on 26666 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26666/26666 [==============================] - 103s - loss: 1.6798 - acc: 0.3901 - val_loss: 1.4510 - val_acc: 0.4768\n",
      "Epoch 2/20\n",
      "26666/26666 [==============================] - 105s - loss: 1.2490 - acc: 0.5530 - val_loss: 1.1757 - val_acc: 0.5832\n",
      "Epoch 3/20\n",
      "26666/26666 [==============================] - 108s - loss: 1.0877 - acc: 0.6139 - val_loss: 1.1447 - val_acc: 0.5973\n",
      "Epoch 4/20\n",
      "26666/26666 [==============================] - 104s - loss: 0.9879 - acc: 0.6529 - val_loss: 1.1050 - val_acc: 0.6139\n",
      "Epoch 5/20\n",
      "26666/26666 [==============================] - 104s - loss: 0.9099 - acc: 0.6804 - val_loss: 1.0512 - val_acc: 0.6432\n",
      "Epoch 6/20\n",
      "26666/26666 [==============================] - 106s - loss: 0.8342 - acc: 0.7066 - val_loss: 1.0109 - val_acc: 0.6546\n",
      "Epoch 7/20\n",
      "26666/26666 [==============================] - 104s - loss: 0.7692 - acc: 0.7320 - val_loss: 0.9962 - val_acc: 0.6577\n",
      "Epoch 8/20\n",
      "26666/26666 [==============================] - 104s - loss: 0.7049 - acc: 0.7532 - val_loss: 1.0078 - val_acc: 0.6600\n",
      "Epoch 9/20\n",
      "26666/26666 [==============================] - 102s - loss: 0.6448 - acc: 0.7777 - val_loss: 0.9677 - val_acc: 0.6801\n",
      "Epoch 10/20\n",
      "26666/26666 [==============================] - 103s - loss: 0.5897 - acc: 0.7959 - val_loss: 1.0213 - val_acc: 0.6661\n",
      "Epoch 11/20\n",
      "26656/26666 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.8136Epoch 00010: early stopping\n",
      "26666/26666 [==============================] - 101s - loss: 0.5387 - acc: 0.8136 - val_loss: 0.9797 - val_acc: 0.6904\n",
      "PREDICT\n",
      "16667/16667 [==============================] - 20s    \n",
      "PREDICT\n",
      "33333/33333 [==============================] - 39s    \n",
      "FIT PARAMS : \n",
      "{'nb_filters_1': 32, 'nb_filters_2': 32, 'nb_hunits': 750, 'filter_size_2': 9, 'pool_size_1': 2, 'pool_size_2': 4, 'filter_size_1': 3}\n",
      "Train on 26667 samples, validate on 6667 samples\n",
      "Epoch 1/20\n",
      "26667/26667 [==============================] - 104s - loss: 1.7068 - acc: 0.3790 - val_loss: 1.4263 - val_acc: 0.4813\n",
      "Epoch 2/20\n",
      "26667/26667 [==============================] - 103s - loss: 1.2585 - acc: 0.5491 - val_loss: 1.2871 - val_acc: 0.5512\n",
      "Epoch 3/20\n",
      " 4256/26667 [===>..........................] - ETA: 78s - loss: 1.1379 - acc: 0.6039"
     ]
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    net = Sequential()\n",
    "    net.add(Convolution2D(hp['nb_filters_1'], hp['filter_size_1'], hp['filter_size_1'], border_mode='same', \n",
    "                          input_shape=(height,width,n_channels)))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_1'],hp['pool_size_1'])))\n",
    "    net.add(Convolution2D(hp['nb_filters_2'], hp['filter_size_2'], hp['filter_size_2'], border_mode='same'))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_2'],hp['pool_size_2'])))\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(output_dim=hp['nb_hunits']))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(Dense(output_dim=n_labels))\n",
    "    net.add(Activation(\"softmax\"))\n",
    "    \n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    "\n",
    "params = {\n",
    "    'nb_filters_1': [16,32,64],\n",
    "    'filter_size_1': [3],\n",
    "    'pool_size_1': [2],\n",
    "    'nb_filters_2': [16,32,64],\n",
    "    'filter_size_2': [3,6,9],\n",
    "    'pool_size_2': [2,4],\n",
    "    'nb_hunits': sp_randint(50,1000)\n",
    "}\n",
    "clf = hyperparameter_optim(Classifier,params, nb_iter=10)\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal set of hyperparameters\n",
    "\n",
    "From a randomized search performed with 15 iterations, the best set of hyperparameters that were found is :\n",
    "- nb_filters_1 = 32\n",
    "- filter_size_1 = 3\n",
    "- pool_size_1 = 2\n",
    "- nb_filters_2 = 32\n",
    "- filter_size_2 = 9\n",
    "- pool_size_2 = 4\n",
    "- nb_hunits = 750\n",
    "\n",
    "Finally, we'll train a model with this architecture on the whole training set and evaluate the classification accuracy on the test set (that has never been used before)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of optimized model and performance assessment on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT PARAMS : \n",
      "{'nb_filters_1': 32, 'nb_filters_2': 32, 'nb_hunits': 750, 'filter_size_2': 9, 'pool_size_1': 2, 'pool_size_2': 4, 'filter_size_1': 3}\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 148s - loss: 1.5336 - acc: 0.4420 - val_loss: 1.2261 - val_acc: 0.5652\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 146s - loss: 1.1258 - acc: 0.6029 - val_loss: 1.0745 - val_acc: 0.6251\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 146s - loss: 0.9806 - acc: 0.6548 - val_loss: 0.9774 - val_acc: 0.6585\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 147s - loss: 0.8857 - acc: 0.6892 - val_loss: 0.9115 - val_acc: 0.6853\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 148s - loss: 0.8076 - acc: 0.7193 - val_loss: 0.9002 - val_acc: 0.6966\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 150s - loss: 0.7353 - acc: 0.7436 - val_loss: 0.8877 - val_acc: 0.6945\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 148s - loss: 0.6730 - acc: 0.7662 - val_loss: 0.8387 - val_acc: 0.7143\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 145s - loss: 0.6147 - acc: 0.7872 - val_loss: 0.9037 - val_acc: 0.6994\n",
      "Epoch 9/20\n",
      "39968/40000 [============================>.] - ETA: 0s - loss: 0.5566 - acc: 0.8081Epoch 00008: early stopping\n",
      "40000/40000 [==============================] - 146s - loss: 0.5566 - acc: 0.8082 - val_loss: 0.8505 - val_acc: 0.7150\n",
      "Detailed classification report:\n",
      "\n",
      "PREDICT\n",
      "10000/10000 [==============================] - 11s    \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.69      0.74      1000\n",
      "          1       0.79      0.85      0.82      1000\n",
      "          2       0.72      0.53      0.61      1000\n",
      "          3       0.53      0.52      0.53      1000\n",
      "          4       0.65      0.68      0.67      1000\n",
      "          5       0.59      0.64      0.61      1000\n",
      "          6       0.77      0.79      0.78      1000\n",
      "          7       0.70      0.80      0.75      1000\n",
      "          8       0.84      0.80      0.82      1000\n",
      "          9       0.74      0.81      0.77      1000\n",
      "\n",
      "avg / total       0.71      0.71      0.71     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Classifier(BaseEstimator):  \n",
    "\n",
    "    def __init__(self, nb_filters_1=32, filter_size_1=3, pool_size_1=2,\n",
    "                 nb_filters_2=32, filter_size_2=9, pool_size_2=4, nb_hunits=750):\n",
    "        self.nb_filters_1 = nb_filters_1\n",
    "        self.filter_size_1 = filter_size_1\n",
    "        self.pool_size_1 = pool_size_1\n",
    "        self.nb_filters_2 = nb_filters_2\n",
    "        self.filter_size_2 = filter_size_2\n",
    "        self.pool_size_2 = pool_size_2\n",
    "        self.nb_hunits = nb_hunits\n",
    "        \n",
    "    def preprocess(self, X):\n",
    "        X = X.reshape((X.shape[0],height,width,n_channels))\n",
    "        X = (X / 255.)\n",
    "        X = X.astype(np.float32)\n",
    "        return X\n",
    "    \n",
    "    def preprocess_y(self, y):\n",
    "        return np_utils.to_categorical(y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = self.preprocess(X)\n",
    "        y = self.preprocess_y(y)\n",
    "        \n",
    "        hyper_parameters = dict(\n",
    "        nb_filters_1 = self.nb_filters_1,\n",
    "        filter_size_1 = self.filter_size_1,\n",
    "        pool_size_1 = self.pool_size_1,\n",
    "        nb_filters_2 = self.nb_filters_2,\n",
    "        filter_size_2 = self.filter_size_2,\n",
    "        pool_size_2 = self.pool_size_2,\n",
    "        nb_hunits = self.nb_hunits\n",
    "        )\n",
    "        \n",
    "        print(\"FIT PARAMS : \")\n",
    "        print(hyper_parameters)\n",
    "        \n",
    "        self.model = build_model(hyper_parameters)\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=1, verbose=1, mode='auto')\n",
    "        self.model.fit(X, y, nb_epoch=20, verbose=1, callbacks=[earlyStopping], validation_split=0.2, \n",
    "                       validation_data=None, shuffle=True)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"PREDICT\")\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict_classes(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        print(\"SCORE\")\n",
    "        print(self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None))\n",
    "        return self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None) \n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    net = Sequential()\n",
    "    net.add(Convolution2D(hp['nb_filters_1'], hp['filter_size_1'], hp['filter_size_1'], border_mode='same', \n",
    "                          input_shape=(height,width,n_channels)))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_1'],hp['pool_size_1'])))\n",
    "    net.add(Convolution2D(hp['nb_filters_2'], hp['filter_size_2'], hp['filter_size_2'], border_mode='same'))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_2'],hp['pool_size_2'])))\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(output_dim=hp['nb_hunits']))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(Dense(output_dim=n_labels))\n",
    "    net.add(Activation(\"softmax\"))\n",
    "    \n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    "\n",
    "\n",
    "clf = Classifier()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performances of our final model\n",
    "With our final model, we reach average precision/recall of 71%. With this very basic model, we would have ranked 65$^{th}$ out of 231 on the Kaggle leaderboard. \n",
    "\n",
    "Focusing on the individual F1-scores for each class, one can notice that the labels for which performances are a bit worse (i.e. F1 < 70%) are : 2,3,4 and 5. Let's have a look at the details of the confusion matrix to get an insight of what's happening with those labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.69      0.74      1000\n",
      "          1       0.79      0.85      0.82      1000\n",
      "          2       0.72      0.53      0.61      1000\n",
      "          3       0.53      0.52      0.53      1000\n",
      "          4       0.65      0.68      0.67      1000\n",
      "          5       0.59      0.64      0.61      1000\n",
      "          6       0.77      0.79      0.78      1000\n",
      "          7       0.70      0.80      0.75      1000\n",
      "          8       0.84      0.80      0.82      1000\n",
      "          9       0.74      0.81      0.77      1000\n",
      "\n",
      "avg / total       0.71      0.71      0.71     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2sAAAMECAYAAAAhF70fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1164f3910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confusion_matrix(y_true,y_pred):\n",
    "    cm_array = confusion_matrix(y_true,y_pred)\n",
    "    true_labels = np.unique(y_true)\n",
    "    pred_labels = np.unique(y_pred)\n",
    "    plt.imshow(cm_array, interpolation='nearest', cmap=plt.cm.jet)\n",
    "    plt.title(\"Confusion matrix\", fontsize=16)\n",
    "    cbar = plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Number of images', rotation=270, labelpad=30, fontsize=12)\n",
    "    xtick_marks = np.arange(len(true_labels))\n",
    "    ytick_marks = np.arange(len(pred_labels))\n",
    "    plt.xticks(xtick_marks, true_labels, rotation=90)\n",
    "    plt.yticks(ytick_marks,pred_labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=14)\n",
    "    plt.xlabel('Predicted label', fontsize=14)\n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 12\n",
    "    fig_size[1] = 12\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "plot_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the confusion matrix\n",
    "Knowing the class labels :\n",
    "0 : airplane \n",
    "1 : automobile \n",
    "2 : bird \n",
    "3 : cat \n",
    "4 : deer \n",
    "5 : dog \n",
    "6 : frog \n",
    "7 : horse \n",
    "8 : ship \n",
    "9 : truck\n",
    "\n",
    "One can notice that the classes for which the F1-score is below 70% all correspond to animals. \n",
    "\n",
    "Some remarkable facts out of the confusion matrix : \n",
    "- Trucks and cars are sometimes mixed up\n",
    "- Birds are the only animals that are sometimes mistakenly classified as planes. The reason behind might be that those two classes are the only ones in which the background is often a blue sky. Hence our model \"deduces\" from the similar background that the bird picture belongs to the \"airplane\" class\n",
    "- Cats are often misclassified as dogs, and the opposite is as well true\n",
    "- Deers are mainly misclassified as horses\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "With minimal efforts, we managed to reach an average F1-score of 71%, which is not that bad for a classification task wtih 10 labels. To improve the performances (the [Kaggle leaderboard](https://www.kaggle.com/c/cifar-10/leaderboard) demonstrates that the mean accuracy can go up to 95%), we could set up more complex model architectures so as to refine the feature extraction. It could also be worth trying some pre-processing on the images, for example finding a way to handle the blue-sky backgrounds that are misleading the classification for planes and brids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
